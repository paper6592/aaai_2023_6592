{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a56df7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0631672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLabelBiasResults(files=[], metric = 'eod'):\n",
    "    plt.figure(figsize=(10,7))\n",
    "    for file in files:\n",
    "        with open(files[file], 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "        x = sorted([float(i.split('_')[1]) for i in results['label_bias'].keys()])\n",
    "        y_bal = []\n",
    "        y_biased = []\n",
    "        y_orig = []\n",
    "        for i in x:\n",
    "            if metric == 'dp':\n",
    "                y_bal.append(abs(results['label_bias'][f'imbalance_{str(i)}']['balanced'].statistical_parity_difference()))\n",
    "                y_biased.append(abs(results['label_bias'][f'imbalance_{str(i)}']['biased'].statistical_parity_difference()))\n",
    "                y_orig.append(abs(results['label_bias'][f'imbalance_{str(i)}']['original'].statistical_parity_difference()))\n",
    "            elif metric == 'acc':\n",
    "                y_bal.append(abs(results['label_bias'][f'imbalance_{str(i)}']['balanced'].accuracy()))\n",
    "                y_biased.append(abs(results['label_bias'][f'imbalance_{str(i)}']['biased'].accuracy()))\n",
    "                y_orig.append(abs(results['label_bias'][f'imbalance_{str(i)}']['original'].accuracy()))\n",
    "            elif metric == 'eod':\n",
    "                y_bal.append(abs(results['label_bias'][f'imbalance_{str(i)}']['balanced'].equal_opportunity_difference()))\n",
    "                y_biased.append(abs(results['label_bias'][f'imbalance_{str(i)}']['biased'].equal_opportunity_difference()))\n",
    "                y_orig.append(abs(results['label_bias'][f'imbalance_{str(i)}']['original'].equal_opportunity_difference()))\n",
    "        if metric == 'acc':\n",
    "            max_y = [min(y_bal[i], y_biased[i], y_orig[i]) for i in range(len(y_bal))]\n",
    "        else:\n",
    "            max_y = [max(y_bal[i], y_biased[i], y_orig[i]) for i in range(len(y_bal))]\n",
    "        #plt.plot(x, y_bal, label=f'Balanced {file}')\n",
    "        #plt.plot(x, y_biased, label=f'Biased {file}')\n",
    "        #plt.plot(x, y_orig, label=f'Original {file}')\n",
    "        plt.plot(x, max_y, label=f'Worst Case {file}')\n",
    "    plt.legend(bbox_to_anchor=(1.0,0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da25608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'adult'\n",
    "plotLabelBiasResults(files={\n",
    "    #'base': f'/media/data_dump/Mohit/neurips2022_data/results/base__eop__{dataset}__lr.pkl',\n",
    "                            #'jiang_nachum': f'/media/data_dump/Mohit/neurips2022_data/results/jiang_nachum__eop__{dataset}__lr.pkl',\n",
    "    'rew':f'/media/data_dump/Mohit/neurips2022_data/results/rew__eop__{dataset}__lr.pkl',\n",
    "    #'exp_grad':f'/media/data_dump/Mohit/neurips2022_data/results/exp_grad__dp__{dataset}__lr.pkl',\n",
    "    #'meta_fair':f'/media/data_dump/Mohit/neurips2022_data/results/meta_fair__fdr__{dataset}__lr.pkl',\n",
    "    \n",
    "                            #'cal_eq':f'/media/data_dump/Mohit/neurips2022_data/results/cal_eq__eop__{dataset}__lr.pkl',\n",
    "    #'eq':f'/media/data_dump/Mohit/neurips2022_data/results/eq__eop__{dataset}__lr.pkl',\n",
    "    'reject':f'/media/data_dump/Mohit/neurips2022_data/results/reject__eop__{dataset}__lr.pkl',\n",
    "}, metric='acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0a9c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'/media/data_dump/Mohit/neurips2022_data/results/cal_eq__eop__adult__lr.pkl','rb') as f:\n",
    "    results = pickle.load(f)\n",
    "plt.figure()\n",
    "for i in range(1,11):\n",
    "    i = i/10\n",
    "    x = []\n",
    "    y = []\n",
    "    for j in range(1,11):\n",
    "        j = j/10\n",
    "        key = f'imbalance_{str(j)}_{str(i)}'\n",
    "        x.append(j)\n",
    "        y.append(max(\n",
    "            abs(results['undersample'][key]['original'].equal_opportunity_difference()),\n",
    "            abs(results['undersample'][key]['balanced'].equal_opportunity_difference()),\n",
    "            abs(results['undersample'][key]['biased'].equal_opportunity_difference()),\n",
    "        ))\n",
    "    plt.plot(x,y, label=f'Beta Negative {str(i)}, Worst case')\n",
    "plt.legend(bbox_to_anchor=(1.0,0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd73c7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotUnderSampleResults(files=[], metric = 'dp', beta_neg=1.0):\n",
    "    plt.figure(figsize=(10,7))\n",
    "    for file in files:\n",
    "        with open(files[file], 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "        x = sorted([float(i.split('_')[1]) for i in results['undersample'].keys()])\n",
    "        y_bal = []\n",
    "        y_biased = []\n",
    "        y_orig = []\n",
    "        for i in x:\n",
    "            if metric == 'dp':\n",
    "                y_bal.append(abs(results['undersample'][f'imbalance_{str(i)}_{str(beta_neg)}']['balanced'].statistical_parity_difference()))\n",
    "                y_biased.append(abs(results['undersample'][f'imbalance_{str(i)}_{str(beta_neg)}']['biased'].statistical_parity_difference()))\n",
    "                y_orig.append(abs(results['undersample'][f'imbalance_{str(i)}_{str(beta_neg)}']['original'].statistical_parity_difference()))\n",
    "            elif metric == 'acc':\n",
    "                y_bal.append(abs(results['undersample'][f'imbalance_{str(i)}_{str(beta_neg)}']['balanced'].accuracy()))\n",
    "                y_biased.append(abs(results['undersample'][f'imbalance_{str(i)}_{str(beta_neg)}']['biased'].accuracy()))\n",
    "                y_orig.append(abs(results['undersample'][f'imbalance_{str(i)}_{str(beta_neg)}']['original'].accuracy()))\n",
    "            elif metric == 'eod':\n",
    "                y_bal.append(abs(results['undersample'][f'imbalance_{str(i)}_{str(beta_neg)}']['balanced'].equal_opportunity_difference()))\n",
    "                y_biased.append(abs(results['undersample'][f'imbalance_{str(i)}_{str(beta_neg)}']['biased'].equal_opportunity_difference()))\n",
    "                y_orig.append(abs(results['undersample'][f'imbalance_{str(i)}_{str(beta_neg)}']['original'].equal_opportunity_difference()))\n",
    "        if metric == 'acc':\n",
    "            max_y = [min(y_bal[i], y_biased[i], y_orig[i]) for i in range(len(y_bal))]\n",
    "        else:\n",
    "            max_y = [max(y_bal[i], y_biased[i], y_orig[i]) for i in range(len(y_bal))]\n",
    "        #plt.plot(x, y_bal, label=f'Balanced {file}')\n",
    "        #plt.plot(x, y_biased, label=f'Biased {file}')\n",
    "        #plt.plot(x, y_orig, label=f'Original {file}')\n",
    "        plt.plot(x, max_y, label=f'Worst Case {file}, beta neg {str(beta_neg)}')\n",
    "    plt.legend(bbox_to_anchor=(1.0,0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab9ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'adult'\n",
    "plotUnderSampleResults(files={\n",
    "    'base': f'/media/data_dump/Mohit/neurips2022_data/results/base__eop__{dataset}__lr.pkl',\n",
    "                            'jiang_nachum': f'/media/data_dump/Mohit/neurips2022_data/results/jiang_nachum__eop__{dataset}__lr.pkl',\n",
    "    'rew':f'/media/data_dump/Mohit/neurips2022_data/results/rew__eop__{dataset}__lr.pkl',\n",
    "    'exp_grad':f'/media/data_dump/Mohit/neurips2022_data/results/exp_grad__dp__{dataset}__lr.pkl',\n",
    "    'meta_fair':f'/media/data_dump/Mohit/neurips2022_data/results/meta_fair__fdr__{dataset}__lr.pkl',\n",
    "    \n",
    "                            'cal_eq':f'/media/data_dump/Mohit/neurips2022_data/results/cal_eq__eop__{dataset}__lr.pkl',\n",
    "    'eq':f'/media/data_dump/Mohit/neurips2022_data/results/eq__eop__{dataset}__lr.pkl',\n",
    "    'reject':f'/media/data_dump/Mohit/neurips2022_data/results/reject__eop__{dataset}__lr.pkl',\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ac7d80",
   "metadata": {},
   "source": [
    "### Mean-Variance plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca04168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset='adult'\n",
    "files={\n",
    "    'base': f'/media/data_dump/Mohit/neurips2022_data/results/base__eop__{dataset}__lr.pkl',\n",
    "                            'jiang_nachum': f'/media/data_dump/Mohit/neurips2022_data/results/jiang_nachum__eop__{dataset}__lr.pkl',\n",
    "    'rew':f'/media/data_dump/Mohit/neurips2022_data/results/rew__eop__{dataset}__lr.pkl',\n",
    "    'exp_grad_eodds':f'/media/data_dump/Mohit/neurips2022_data/results/exp_grad__eodds__{dataset}__lr.pkl',\n",
    "    'meta_fair_fdr':f'/media/data_dump/Mohit/neurips2022_data/results/meta_fair__fdr__{dataset}__lr.pkl',\n",
    "    \n",
    "                            'cal_eq':f'/media/data_dump/Mohit/neurips2022_data/results/cal_eq__eop__{dataset}__lr.pkl',\n",
    "    'eq':f'/media/data_dump/Mohit/neurips2022_data/results/eq__eop__{dataset}__lr.pkl',\n",
    "    'reject':f'/media/data_dump/Mohit/neurips2022_data/results/reject__eop__{dataset}__lr.pkl',\n",
    "}\n",
    "for i in files:\n",
    "    with open(files[i], 'rb') as f:\n",
    "        files[i] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4443af56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def meanVariancePlots(files=[], metric = 'dp', beta_neg=1.0, testSet='bal'):\n",
    "    #plt.figure(figsize=(10,7))\n",
    "    y_means = []\n",
    "    y_errs  = []\n",
    "    x_means = {}\n",
    "    for file in files:\n",
    "        #with open(files[file], 'rb') as f:\n",
    "        #    results = pickle.load(f)\n",
    "        results = files[file]\n",
    "        x = sorted([float(i.split('_')[1]) for i in results['undersample'].keys()])\n",
    "        y_bal = []\n",
    "        y_biased = []\n",
    "        y_orig = []\n",
    "        for i in x:\n",
    "            if metric == 'dp':\n",
    "                y_bal.append(abs(results['undersample'][f'imbalance_{str(i)}_{str(beta_neg)}']['balanced'].statistical_parity_difference()))\n",
    "                y_biased.append(abs(results['undersample'][f'imbalance_{str(i)}_{str(beta_neg)}']['biased'].statistical_parity_difference()))\n",
    "                y_orig.append(abs(results['undersample'][f'imbalance_{str(i)}_{str(beta_neg)}']['original'].statistical_parity_difference()))\n",
    "            elif metric == 'acc':\n",
    "                y_bal.append(abs(results['undersample'][f'imbalance_{str(i)}_{str(beta_neg)}']['balanced'].accuracy()))\n",
    "                y_biased.append(abs(results['undersample'][f'imbalance_{str(i)}_{str(beta_neg)}']['biased'].accuracy()))\n",
    "                y_orig.append(abs(results['undersample'][f'imbalance_{str(i)}_{str(beta_neg)}']['original'].accuracy()))\n",
    "            elif metric == 'eod':\n",
    "                y_bal.append(abs(results['undersample'][f'imbalance_{str(i)}_{str(beta_neg)}']['balanced'].equal_opportunity_difference()))\n",
    "                y_biased.append(abs(results['undersample'][f'imbalance_{str(i)}_{str(beta_neg)}']['biased'].equal_opportunity_difference()))\n",
    "                y_orig.append(abs(results['undersample'][f'imbalance_{str(i)}_{str(beta_neg)}']['original'].equal_opportunity_difference()))\n",
    "        if metric == 'acc':\n",
    "            max_y = [1.0 - min(y_bal[i], y_biased[i], y_orig[i]) for i in range(len(y_bal))]\n",
    "            y_bal = [1.0 - i for i in y_bal]\n",
    "            y_orig = [1.0 - i for i in y_orig]\n",
    "            y_biased = [1.0 - i for i in y_biased]\n",
    "        else:\n",
    "            max_y = [max(y_bal[i], y_biased[i], y_orig[i]) for i in range(len(y_bal))]\n",
    "        if testSet == 'bal':\n",
    "            mean_val = np.mean(y_bal)\n",
    "            variance = np.var(y_bal)\n",
    "        elif testSet == 'biased':\n",
    "            mean_val = np.mean(y_biased)\n",
    "            variance = np.var(y_biased)\n",
    "        elif testSet == 'orig':\n",
    "            mean_val = np.mean(y_orig)\n",
    "            variance = np.var(y_orig)\n",
    "        elif testSet == 'avg':\n",
    "            y_avg =  [np.mean([y_bal[i], y_biased[i], y_orig[i]]) for i in range(len(y_bal))]\n",
    "            mean_val = np.mean(y_avg)\n",
    "            variance = np.var(y_avg)\n",
    "        #plt.plot(x, max_y, label=f'Worst Case {file}, beta neg {str(beta_neg)}')\n",
    "        #y_means.append(mean_val)\n",
    "        #y_errs.append(variance)\n",
    "        x_means[file] = [mean_val, variance]\n",
    "    #plt.errorbar(x_means, y_means, y_errs, fmt='o', color='black',ecolor='lightgray', elinewidth=3, capsize=0)\n",
    "    return x_means\n",
    "    #plt.legend(bbox_to_anchor=(1.0,0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edb2fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for beta_neg in range(1,11):\n",
    "    beta_neg = beta_neg/10\n",
    "    dataset = 'adult'\n",
    "    testSet = 'avg'\n",
    "    eop_log = meanVariancePlots(files=files, metric='eod', beta_neg=beta_neg, testSet=testSet)\n",
    "    acc_log = meanVariancePlots(files=files, metric='acc', beta_neg=beta_neg, testSet=testSet)\n",
    "    plt.figure()\n",
    "    for i in eop_log:\n",
    "        plt.errorbar(y=acc_log[i][0], x=eop_log[i][0], xerr=eop_log[i][1], yerr=acc_log[i][1], label=i, fmt='o')\n",
    "    plt.axvline(x=eop_log['base'][0])\n",
    "    plt.axhline(y=acc_log['base'][0])\n",
    "    plt.xlabel(f'Equal Opportunity')\n",
    "    plt.ylabel(f'Error Rate')\n",
    "    plt.title(f'Test Set {testSet}, Beta Negative {beta_neg}')\n",
    "    plt.legend(bbox_to_anchor=(1.5,1.0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278af2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mpl_toolkits import mplot3d\n",
    "# fig = plt.figure()\n",
    "# ax = plt.axes(projection='3d')\n",
    "# for i in eop_log:\n",
    "#     temp_x = []\n",
    "#     temp_y = []\n",
    "#     temp_z = []\n",
    "#     for beta_neg in range(1,11):\n",
    "#         beta_neg = beta_neg/10\n",
    "#         dataset = 'adult'\n",
    "#         eop_log = meanVariancePlots(files=files, metric='eod', beta_neg=beta_neg)\n",
    "#         acc_log = meanVariancePlots(files=files, metric='acc', beta_neg=beta_neg) \n",
    "#         temp_x.append(acc_log[i][0]) \n",
    "#         temp_y.append(eop_log[i][0])\n",
    "#         temp_z.append(beta_neg)\n",
    "#         #plt.errorbar(x=acc_log[i][0], y=eop_log[i][0], yerr=eop_log[i][1], xerr=acc_log[i][1], label=i)\n",
    "#     ax.plot3D(temp_x, temp_y, temp_z, label=i)\n",
    "# ax.view_init(-120, 60)\n",
    "# ax.set_xlabel('Error Rate')\n",
    "# ax.set_ylabel('Equal Opportunity difference')\n",
    "# ax.set_zlabel('Beta Negative')\n",
    "# ax.legend(bbox_to_anchor=(1.5,1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ac5bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "testSet = 'avg'\n",
    "for i in eop_log:\n",
    "    temp_x = []\n",
    "    temp_y = []\n",
    "    temp_z = []\n",
    "    for beta_neg in range(1,11):\n",
    "        beta_neg = beta_neg/10\n",
    "        eop_log = meanVariancePlots(files=files, metric='eod', beta_neg=beta_neg, testSet=testSet)\n",
    "        acc_log = meanVariancePlots(files=files, metric='acc', beta_neg=beta_neg, testSet=testSet) \n",
    "        temp_x.append(acc_log[i][0])\n",
    "        temp_y.append(eop_log[i][0])\n",
    "        #plt.errorbar(x=acc_log[i][0], y=eop_log[i][0], yerr=eop_log[i][1], xerr=acc_log[i][1], label=i)\n",
    "    plt.scatter(np.mean(temp_x), np.mean(temp_y), label=i)\n",
    "    if i == 'base':\n",
    "        plt.axvline(x=np.mean(temp_x))\n",
    "        plt.axhline(y=np.mean(temp_y))\n",
    "plt.xlabel('Error Rate')\n",
    "plt.ylabel('Equal Opportunity difference')\n",
    "plt.title(f'Overal Average, Test Set: {testSet}')\n",
    "plt.legend(bbox_to_anchor=(1.5,1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf8135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnames = {\n",
    "'aliceblue':            '#F0F8FF',\n",
    "'aqua':                 '#00FFFF',\n",
    "'aquamarine':           '#7FFFD4',\n",
    "'azure':                '#F0FFFF',\n",
    "'beige':                '#F5F5DC',\n",
    "'bisque':               '#FFE4C4',\n",
    "'black':                '#000000',\n",
    "'blanchedalmond':       '#FFEBCD',\n",
    "'blue':                 '#0000FF',\n",
    "'blueviolet':           '#8A2BE2',\n",
    "'brown':                '#A52A2A',\n",
    "'burlywood':            '#DEB887',\n",
    "'cadetblue':            '#5F9EA0',\n",
    "'chartreuse':           '#7FFF00',\n",
    "'chocolate':            '#D2691E',\n",
    "'coral':                '#FF7F50',\n",
    "'cornflowerblue':       '#6495ED',\n",
    "'cornsilk':             '#FFF8DC',\n",
    "'crimson':              '#DC143C',\n",
    "'cyan':                 '#00FFFF',\n",
    "'darkblue':             '#00008B',\n",
    "'darkcyan':             '#008B8B',\n",
    "'darkgoldenrod':        '#B8860B',\n",
    "'darkgray':             '#A9A9A9',\n",
    "'darkgreen':            '#006400',\n",
    "'darkkhaki':            '#BDB76B',\n",
    "'darkmagenta':          '#8B008B',\n",
    "'darkolivegreen':       '#556B2F',\n",
    "'darkorange':           '#FF8C00',\n",
    "'darkorchid':           '#9932CC',\n",
    "'darkred':              '#8B0000',\n",
    "'darksalmon':           '#E9967A',\n",
    "'darkseagreen':         '#8FBC8F',\n",
    "'darkslateblue':        '#483D8B',\n",
    "'darkslategray':        '#2F4F4F',\n",
    "'darkturquoise':        '#00CED1',\n",
    "'darkviolet':           '#9400D3',\n",
    "'deeppink':             '#FF1493',\n",
    "'deepskyblue':          '#00BFFF',\n",
    "'dimgray':              '#696969',\n",
    "'dodgerblue':           '#1E90FF',\n",
    "'firebrick':            '#B22222',\n",
    "'floralwhite':          '#FFFAF0',\n",
    "'forestgreen':          '#228B22',\n",
    "'fuchsia':              '#FF00FF',\n",
    "'gainsboro':            '#DCDCDC',\n",
    "'ghostwhite':           '#F8F8FF',\n",
    "'gold':                 '#FFD700',\n",
    "'goldenrod':            '#DAA520',\n",
    "'gray':                 '#808080',\n",
    "'green':                '#008000',\n",
    "'greenyellow':          '#ADFF2F',\n",
    "'honeydew':             '#F0FFF0',\n",
    "'hotpink':              '#FF69B4',\n",
    "'indianred':            '#CD5C5C',\n",
    "'indigo':               '#4B0082',\n",
    "'ivory':                '#FFFFF0',\n",
    "'khaki':                '#F0E68C',\n",
    "'lavender':             '#E6E6FA',\n",
    "'lavenderblush':        '#FFF0F5',\n",
    "'lawngreen':            '#7CFC00',\n",
    "'lemonchiffon':         '#FFFACD',\n",
    "'lightblue':            '#ADD8E6',\n",
    "'lightcoral':           '#F08080',\n",
    "'lightcyan':            '#E0FFFF',\n",
    "'lightgoldenrodyellow': '#FAFAD2',\n",
    "'lightgreen':           '#90EE90',\n",
    "'lightgray':            '#D3D3D3',\n",
    "'lightpink':            '#FFB6C1',\n",
    "'lightsalmon':          '#FFA07A',\n",
    "'lightseagreen':        '#20B2AA',\n",
    "'lightskyblue':         '#87CEFA',\n",
    "'lightslategray':       '#778899',\n",
    "'lightsteelblue':       '#B0C4DE',\n",
    "'lightyellow':          '#FFFFE0',\n",
    "'lime':                 '#00FF00',\n",
    "'limegreen':            '#32CD32',\n",
    "'linen':                '#FAF0E6',\n",
    "'magenta':              '#FF00FF',\n",
    "'maroon':               '#800000',\n",
    "'mediumaquamarine':     '#66CDAA',\n",
    "'mediumblue':           '#0000CD',\n",
    "'mediumorchid':         '#BA55D3',\n",
    "'mediumpurple':         '#9370DB',\n",
    "'mediumseagreen':       '#3CB371',\n",
    "'mediumslateblue':      '#7B68EE',\n",
    "'mediumspringgreen':    '#00FA9A',\n",
    "'mediumturquoise':      '#48D1CC',\n",
    "'mediumvioletred':      '#C71585',\n",
    "'midnightblue':         '#191970',\n",
    "'mintcream':            '#F5FFFA',\n",
    "'mistyrose':            '#FFE4E1',\n",
    "'moccasin':             '#FFE4B5',\n",
    "'navajowhite':          '#FFDEAD',\n",
    "'navy':                 '#000080',\n",
    "'oldlace':              '#FDF5E6',\n",
    "'olive':                '#808000',\n",
    "'olivedrab':            '#6B8E23',\n",
    "'orange':               '#FFA500',\n",
    "'orangered':            '#FF4500',\n",
    "'orchid':               '#DA70D6',\n",
    "'palegoldenrod':        '#EEE8AA',\n",
    "'palegreen':            '#98FB98',\n",
    "'paleturquoise':        '#AFEEEE',\n",
    "'palevioletred':        '#DB7093',\n",
    "'papayawhip':           '#FFEFD5',\n",
    "'peachpuff':            '#FFDAB9',\n",
    "'peru':                 '#CD853F',\n",
    "'pink':                 '#FFC0CB',\n",
    "'plum':                 '#DDA0DD',\n",
    "'powderblue':           '#B0E0E6',\n",
    "'purple':               '#800080',\n",
    "'red':                  '#FF0000',\n",
    "'rosybrown':            '#BC8F8F',\n",
    "'royalblue':            '#4169E1',\n",
    "'saddlebrown':          '#8B4513',\n",
    "'salmon':               '#FA8072',\n",
    "'sandybrown':           '#FAA460',\n",
    "'seagreen':             '#2E8B57',\n",
    "'seashell':             '#FFF5EE',\n",
    "'sienna':               '#A0522D',\n",
    "'silver':               '#C0C0C0',\n",
    "'skyblue':              '#87CEEB',\n",
    "'slateblue':            '#6A5ACD',\n",
    "'slategray':            '#708090',\n",
    "'snow':                 '#FFFAFA',\n",
    "'springgreen':          '#00FF7F',\n",
    "'steelblue':            '#4682B4',\n",
    "'tan':                  '#D2B48C',\n",
    "'teal':                 '#008080',\n",
    "'thistle':              '#D8BFD8',\n",
    "'tomato':               '#FF6347',\n",
    "'turquoise':            '#40E0D0',\n",
    "'violet':               '#EE82EE',\n",
    "'wheat':                '#F5DEB3',\n",
    "'white':                '#FFFFFF',\n",
    "'whitesmoke':           '#F5F5F5',\n",
    "'yellow':               '#FFFF00',\n",
    "'yellowgreen':          '#9ACD32'}\n",
    "#pallates = ['blue', 'red', 'green', 'black', 'yellow', 'brown', 'violet', ]\n",
    "colors = {}\n",
    "for i, file in enumerate(files):\n",
    "    colors[file] = list(cnames.keys())[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3c831e",
   "metadata": {},
   "source": [
    "### Subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14367c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72ca93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeUndersamplePlots(files, betaNegFactors=[0.1, 0.3, 0.7, 1.0], metrics=['dp', 'eod'], testSet='orig', omitCls=['meta_fair_fdr']):\n",
    "    axisTitles = {\n",
    "        'dp': 'Statistical Parity Diff.',\n",
    "        'eod': 'Equal Oppor. Diff.'\n",
    "    }\n",
    "    fig, ax = plt.subplots(nrows=len(metrics), ncols=len(betaNegFactors))\n",
    "    fig.subplots_adjust(hspace=0.5, wspace=0.4)\n",
    "    fig.set_figheight(5)\n",
    "    fig.set_figwidth(15)\n",
    "    for m,metric in enumerate(metrics):\n",
    "        for b,beta_neg in enumerate(betaNegFactors):\n",
    "            ax[m,b].title.set_text(f'Metric: {metric}, Beta_Neg: {beta_neg}')\n",
    "            # Calculating required Quantities\n",
    "            y_means = []\n",
    "            y_errs  = []\n",
    "            x_means = {}\n",
    "            for file in files:\n",
    "                if file in omitCls:\n",
    "                    continue\n",
    "                results = files[file]\n",
    "                x = sorted([float(i.split('_')[1]) for i in results['undersample'].keys()])\n",
    "                y_bal = []\n",
    "                y_biased = []\n",
    "                y_orig = []\n",
    "                x_err_rate_bal = []\n",
    "                x_err_rate_biased = []\n",
    "                x_err_rate_orig = []\n",
    "                for i in x:\n",
    "                    if metric == 'dp':\n",
    "                        y_bal.append(abs(results['undersample'][f'imbalance_{str(i)}_{str(beta_neg)}']['balanced'].statistical_parity_difference()))\n",
    "                        y_biased.append(abs(results['undersample'][f'imbalance_{str(i)}_{str(beta_neg)}']['biased'].statistical_parity_difference()))\n",
    "                        y_orig.append(abs(results['undersample'][f'imbalance_{str(i)}_{str(beta_neg)}']['original'].statistical_parity_difference()))\n",
    "                    elif metric == 'eod':\n",
    "                        y_bal.append(abs(results['undersample'][f'imbalance_{str(i)}_{str(beta_neg)}']['balanced'].equal_opportunity_difference()))\n",
    "                        y_biased.append(abs(results['undersample'][f'imbalance_{str(i)}_{str(beta_neg)}']['biased'].equal_opportunity_difference()))\n",
    "                        y_orig.append(abs(results['undersample'][f'imbalance_{str(i)}_{str(beta_neg)}']['original'].equal_opportunity_difference()))\n",
    "                    # Calculate accuracy\n",
    "                    x_err_rate_bal.append(1 - abs(results['undersample'][f'imbalance_{str(i)}_{str(beta_neg)}']['balanced'].accuracy()))\n",
    "                    x_err_rate_biased.append(1 - abs(results['undersample'][f'imbalance_{str(i)}_{str(beta_neg)}']['biased'].accuracy()))\n",
    "                    x_err_rate_orig.append(1 - abs(results['undersample'][f'imbalance_{str(i)}_{str(beta_neg)}']['original'].accuracy()))\n",
    "                if testSet == 'bal':\n",
    "                    #mean_val_y = np.mean(y_bal)\n",
    "                    #min_val_y = np.min(y_bal)\n",
    "                    #max_val_y = np.max(y_bal)\n",
    "                    #mean_val_x = np.mean(x_err_rate_bal)\n",
    "                    #min_val_x = np.min(x_err_rate_bal)\n",
    "                    #max_val_x = np.max(x_err_rate_bal)\n",
    "                    mean_val_y = np.mean(y_bal)\n",
    "                    var_val_y = np.var(y_bal)\n",
    "                    mean_val_x = np.mean(x_err_rate_bal)\n",
    "                    var_val_x = np.var(x_err_rate_bal)\n",
    "                elif testSet == 'biased':\n",
    "                    #mean_val_y = np.mean(y_biased)\n",
    "                    #min_val_y = np.min(y_biased)\n",
    "                    #max_val_y = np.max(y_biased)\n",
    "                    #mean_val_x = np.mean(x_err_rate_biased)\n",
    "                    #min_val_x = np.min(x_err_rate_biased)\n",
    "                    #max_val_x = np.max(x_err_rate_biased)\n",
    "                    mean_val_y = np.mean(y_biased)\n",
    "                    var_val_y = np.var(y_biased)\n",
    "                    mean_val_x = np.mean(x_err_rate_biased)\n",
    "                    var_val_x = np.var(x_err_rate_biased)\n",
    "                elif testSet == 'orig':\n",
    "                    #mean_val_y = np.mean(y_orig)\n",
    "                    #min_val_y = np.min(y_orig)\n",
    "                    #max_val_y = np.max(y_orig)\n",
    "                    #mean_val_x = np.mean(x_err_rate_orig)\n",
    "                    #min_val_x = np.min(x_err_rate_orig)\n",
    "                    #max_val_x = np.max(x_err_rate_orig)\n",
    "                    mean_val_y = np.mean(y_orig)\n",
    "                    var_val_y = np.var(y_orig)\n",
    "                    mean_val_x = np.mean(x_err_rate_orig)\n",
    "                    var_val_x = np.var(x_err_rate_orig)\n",
    "                #plt.plot(x, max_y, label=f'Worst Case {file}, beta neg {str(beta_neg)}')\n",
    "                #y_means.append(mean_val)\n",
    "                #y_errs.append(variance)\n",
    "                #x_means[file] = [mean_val_y, min_val_y, max_val_y, mean_val_x, min_val_x, max_val_x]\n",
    "                x_means[file] = [mean_val_y, var_val_y, mean_val_x, var_val_x]\n",
    "            #print(f'{metric}, {beta_neg}')\n",
    "            #print(x_means)\n",
    "            #print()\n",
    "            dots_x = []\n",
    "            dots_y = []\n",
    "            #lower_x = []\n",
    "            #lower_y = []\n",
    "            #upper_x = []\n",
    "            #upper_y = []\n",
    "            labels = []\n",
    "            var_x = []\n",
    "            var_y = []\n",
    "            for i in x_means:\n",
    "                dots_x.append(x_means[i][2])\n",
    "                dots_y.append(x_means[i][0])\n",
    "                var_x.append(x_means[i][3])\n",
    "                var_y.append(x_means[i][1])\n",
    "                #lower_x.append(x_means[i][4])\n",
    "                #lower_y.append(x_means[i][1])\n",
    "                #upper_x.append(x_means[i][5])\n",
    "                #upper_y.append(x_means[i][2])\n",
    "                labels.append(i)\n",
    "                #ax[m,b].errorbar(x=dots_x, y=dots_y, yerr=var_y, xerr=var_x, label=labels, fmt='o')\n",
    "                ax[m,b].errorbar(x=x_means[i][2], y=x_means[i][0], yerr=x_means[i][1], xerr=x_means[i][3], label=i, fmt='o')\n",
    "            ax[m,b].set_xlim([-0.1,0.8])\n",
    "            ax[m,b].set_ylim([-0.1,0.9])\n",
    "            ax[m,b].axvline(x=x_means['base'][2])\n",
    "            ax[m,b].axhline(y=x_means['base'][0])\n",
    "            ax[m,b].set_xlabel('Error Rate')\n",
    "            ax[m,b].set_ylabel(axisTitles[metric])\n",
    "    plt.xlabel('Error Rate')\n",
    "    plt.legend(bbox_to_anchor=(1.9,1.8))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a4d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def makeGridUndersamplePlots(files, betaNegFactors=[0.1, 0.3, 0.7, 1.0], betaPosFactors = [0.1, 0.3,0.7, 1.0], metric='eod', testSet='orig', dataset=None):\n",
    "    if dataset == 'compas':\n",
    "        omitCls = ['meta_fair_fdr']\n",
    "    else:\n",
    "        omitCls = []\n",
    "    axisTitles = {\n",
    "        'dp': 'Statistical Parity Diff.',\n",
    "        'eod': 'Equal Oppor. Diff.'\n",
    "    }\n",
    "    markers = {\n",
    "        'base': '+',\n",
    "        'jiang_nachum': 'o',\n",
    "        'rew':'o',\n",
    "        'fairgan':'o',\n",
    "        'exp_grad_eodds':'^',\n",
    "        'meta_fair_fdr':'^',\n",
    "        'prej_remover':'^',\n",
    "        'gerry_fair': '^',\n",
    "        'cal_eq':'s',\n",
    "        'eq':'s',\n",
    "        'reject':'s'\n",
    "    }\n",
    "    fig, ax = plt.subplots(nrows=len(betaPosFactors), ncols=len(betaNegFactors))\n",
    "    fig.subplots_adjust(hspace=0.5, wspace=0.4)\n",
    "    fig.set_figheight(13)\n",
    "    fig.set_figwidth(13)\n",
    "    for bp,beta_pos in enumerate(betaPosFactors):\n",
    "        for bn,beta_neg in enumerate(betaNegFactors):\n",
    "            ax[bp,bn].title.set_text(f'Beta_Pos: {beta_pos}, Beta_Neg: {beta_neg}')\n",
    "            # Calculating required Quantities\n",
    "            y_means = []\n",
    "            y_errs  = []\n",
    "            x_means = {}\n",
    "            for file in files:\n",
    "                if file in omitCls:\n",
    "                    continue\n",
    "                results = files[file]\n",
    "                base_results = files['base']\n",
    "                #x = sorted([float(i.split('_')[1]) for i in results['undersample'].keys()])\n",
    "                if metric == 'dp':\n",
    "                    y_bal = abs(results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['balanced'].statistical_parity_difference())/abs(base_results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['balanced'].statistical_parity_difference())\n",
    "                    y_biased = abs(results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['biased'].statistical_parity_difference())/abs(base_results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['biased'].statistical_parity_difference())\n",
    "                    y_orig = abs(results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['original'].statistical_parity_difference())/abs(base_results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['original'].statistical_parity_difference())\n",
    "                elif metric == 'eod':\n",
    "                    y_bal = abs(results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['balanced'].equal_opportunity_difference())/abs(base_results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['balanced'].equal_opportunity_difference())\n",
    "                    y_biased = abs(results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['biased'].equal_opportunity_difference())/abs(base_results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['biased'].equal_opportunity_difference())\n",
    "                    y_orig = abs(results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['original'].equal_opportunity_difference())/abs(base_results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['original'].equal_opportunity_difference())\n",
    "                # Calculate accuracy\n",
    "                x_err_rate_bal = (1 - abs(results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['balanced'].accuracy()))/(1 - abs(base_results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['balanced'].accuracy()))\n",
    "                x_err_rate_biased = (1 - abs(results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['biased'].accuracy()))/(1 - abs(base_results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['biased'].accuracy()))\n",
    "                x_err_rate_orig = (1 - abs(results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['original'].accuracy()))/(1 - abs(base_results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['original'].accuracy()))\n",
    "                if testSet == 'bal':\n",
    "                    y = y_bal\n",
    "                    x = x_err_rate_bal\n",
    "                elif testSet == 'biased':\n",
    "                    y = y_biased\n",
    "                    x = x_err_rate_biased\n",
    "                elif testSet == 'orig':\n",
    "                    y = y_orig\n",
    "                    x = x_err_rate_orig\n",
    "                x_means[file] = [y,x]\n",
    "            dots_x = []\n",
    "            dots_y = []\n",
    "            labels = []\n",
    "            for i in x_means:\n",
    "                dots_x.append(x_means[i][1])\n",
    "                dots_y.append(x_means[i][0])\n",
    "                labels.append(i)\n",
    "                ax[bp,bn].errorbar(x=x_means[i][1], y=x_means[i][0], label=i, fmt=markers[i])\n",
    "            if dataset == 'adult':\n",
    "                ax[bp,bn].set_xlim([-1,4])\n",
    "                ax[bp,bn].set_ylim([-1,4])\n",
    "            elif dataset == 'bank':\n",
    "                ax[bp,bn].set_xlim([-1,4])\n",
    "                ax[bp,bn].set_ylim([-1,4])\n",
    "            elif dataset == 'compas':\n",
    "                ax[bp,bn].set_xlim([-1,4])\n",
    "                ax[bp,bn].set_ylim([-1,4])\n",
    "            elif dataset == 'credit':\n",
    "                ax[bp,bn].set_xlim([-0.2,1.5])\n",
    "                ax[bp,bn].set_ylim([-0.2,1.5])\n",
    "            ax[bp,bn].axvline(x=x_means['base'][1])\n",
    "            ax[bp,bn].axhline(y=x_means['base'][0])\n",
    "            # D2 distribution lines\n",
    "            if metric == 'eod':\n",
    "                ax[bp,bn].axvline(x=(1.0 - base_results['undersample']['imbalance_1.0_1.0']['original'].accuracy())/(1 - abs(base_results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['original'].accuracy())), color='r')\n",
    "                ax[bp,bn].axhline(y=abs(base_results['undersample']['imbalance_1.0_1.0']['original'].equal_opportunity_difference())/abs(base_results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['original'].equal_opportunity_difference()), color='r')\n",
    "            ax[bp,bn].set_xlabel('Error Rate')\n",
    "            ax[bp,bn].set_ylabel(axisTitles[metric])\n",
    "    plt.legend(bbox_to_anchor=(2.1,3.3))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deed4bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "try:\n",
    "    del files\n",
    "    gc.collect()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "dataset='adult'\n",
    "if dataset == 'bank' or dataset =='synthetic':\n",
    "    files={\n",
    "        'base': f'/media/data_dump/Mohit/neurips2022_data/results/base__eop__{dataset}__lr.pkl',\n",
    "        'jiang_nachum': f'/media/data_dump/Mohit/neurips2022_data/results/jiang_nachum__eop__{dataset}__lr.pkl',\n",
    "        'rew':f'/media/data_dump/Mohit/neurips2022_data/results/rew__eop__{dataset}__lr.pkl',\n",
    "        'exp_grad_eodds':f'/media/data_dump/Mohit/neurips2022_data/results/exp_grad__eodds__{dataset}__lr.pkl',\n",
    "        'meta_fair_fdr':f'/media/data_dump/Mohit/neurips2022_data/results/meta_fair__fdr__{dataset}__lr.pkl',\n",
    "        'prej_remover':f'/media/data_dump/Mohit/neurips2022_data/results/prej_remover__eop__{dataset}__lr.pkl',\n",
    "        'gerry_fair': f'/media/data_dump/Mohit/neurips2022_data/results/gerry_fair__eop__{dataset}__lr.pkl',\n",
    "        'cal_eq':f'/media/data_dump/Mohit/neurips2022_data/results/cal_eq__eop__{dataset}__lr.pkl',\n",
    "        'eq':f'/media/data_dump/Mohit/neurips2022_data/results/eq__eop__{dataset}__lr.pkl',\n",
    "        'reject':f'/media/data_dump/Mohit/neurips2022_data/results/reject__eop__{dataset}__lr.pkl',\n",
    "    }\n",
    "else:\n",
    "    files={\n",
    "        'base': f'/media/data_dump/Mohit/neurips2022_data/results/base__eop__{dataset}__lr.pkl',\n",
    "        'jiang_nachum': f'/media/data_dump/Mohit/neurips2022_data/results/jiang_nachum__eop__{dataset}__lr.pkl',\n",
    "        'rew':f'/media/data_dump/Mohit/neurips2022_data/results/rew__eop__{dataset}__lr.pkl',\n",
    "        'fairgan':f'/media/data_dump/Mohit/neurips2022_data/results/fairgan__eop__{dataset}__lr.pkl',\n",
    "        'exp_grad_eodds':f'/media/data_dump/Mohit/neurips2022_data/results/exp_grad__eodds__{dataset}__lr.pkl',\n",
    "        'meta_fair_fdr':f'/media/data_dump/Mohit/neurips2022_data/results/meta_fair__fdr__{dataset}__lr.pkl',\n",
    "        'prej_remover':f'/media/data_dump/Mohit/neurips2022_data/results/prej_remover__eop__{dataset}__lr.pkl',\n",
    "        'gerry_fair': f'/media/data_dump/Mohit/neurips2022_data/results/gerry_fair__eop__{dataset}__lr.pkl',\n",
    "        'cal_eq':f'/media/data_dump/Mohit/neurips2022_data/results/cal_eq__eop__{dataset}__lr.pkl',\n",
    "        'eq':f'/media/data_dump/Mohit/neurips2022_data/results/eq__eop__{dataset}__lr.pkl',\n",
    "        'reject':f'/media/data_dump/Mohit/neurips2022_data/results/reject__eop__{dataset}__lr.pkl',\n",
    "    }\n",
    "for i in files:\n",
    "    with open(files[i], 'rb') as f:\n",
    "        files[i] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2a29d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def final_makeGridUndersamplePlots(files, betaNegFactors=[0.1, 0.3, 0.7, 1.0], betaPosFactors = [0.1, 0.3,0.7, 1.0], metric='eod', testSet='orig', dataset=dataset):\n",
    "    if dataset == 'compas':\n",
    "        omitCls = ['meta_fair_fdr']\n",
    "    else:\n",
    "        omitCls = []\n",
    "    axisTitles = {\n",
    "        'dp': 'Statistical Parity Diff.',\n",
    "        'eod': 'Equal Oppor. Diff.'\n",
    "    }\n",
    "    markers = {\n",
    "        'base': '+',\n",
    "        'jiang_nachum': 'o',\n",
    "        'rew':'o',\n",
    "        'fairgan':'o',\n",
    "        'exp_grad_eodds':'^',\n",
    "        'meta_fair_fdr':'^',\n",
    "        'prej_remover':'^',\n",
    "        'gerry_fair': '^',\n",
    "        'cal_eq':'s',\n",
    "        'eq':'s',\n",
    "        'reject':'s'\n",
    "    }\n",
    "    fig, ax = plt.subplots(nrows=len(betaPosFactors), ncols=len(betaNegFactors))\n",
    "    fig.subplots_adjust(hspace=0.5, wspace=0.4)\n",
    "    fig.set_figheight(13)\n",
    "    fig.set_figwidth(13)\n",
    "    overallStats = {}\n",
    "    for file in files:\n",
    "        if file not in omitCls:\n",
    "            overallStats[file] = [[],[]]\n",
    "    for bp,beta_pos in enumerate(betaPosFactors):\n",
    "        for bn,beta_neg in enumerate(betaNegFactors):\n",
    "            ax[bp,bn].title.set_text(f'Beta_Pos: {beta_pos}, Beta_Neg: {beta_neg}')\n",
    "            # Calculating required Quantities\n",
    "            y_means = []\n",
    "            y_errs  = []\n",
    "            x_means = {}\n",
    "            for file in files:\n",
    "                if file in omitCls:\n",
    "                    continue\n",
    "                results = files[file]\n",
    "                base_results = files['base']\n",
    "                #x = sorted([float(i.split('_')[1]) for i in results['undersample'].keys()])\n",
    "                if metric == 'dp':\n",
    "                    y_bal = abs(results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['balanced'].statistical_parity_difference())\n",
    "                    y_biased = abs(results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['biased'].statistical_parity_difference())\n",
    "                    y_orig = abs(results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['original'].statistical_parity_difference())\n",
    "                elif metric == 'eod':\n",
    "                    y_bal = abs(results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['balanced'].equal_opportunity_difference())\n",
    "                    y_biased = abs(results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['biased'].equal_opportunity_difference())\n",
    "                    y_orig = abs(results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['original'].equal_opportunity_difference())\n",
    "                # Calculate accuracy\n",
    "                x_err_rate_bal = (1 - abs(results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['balanced'].accuracy()))\n",
    "                x_err_rate_biased = (1 - abs(results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['biased'].accuracy()))\n",
    "                x_err_rate_orig = (1 - abs(results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['original'].accuracy()))\n",
    "                if testSet == 'bal':\n",
    "                    y = y_bal\n",
    "                    x = x_err_rate_bal\n",
    "                elif testSet == 'biased':\n",
    "                    y = y_biased\n",
    "                    x = x_err_rate_biased\n",
    "                elif testSet == 'orig':\n",
    "                    y = y_orig\n",
    "                    x = x_err_rate_orig\n",
    "                x_means[file] = [y,x]\n",
    "                overallStats[file][0].append(x)\n",
    "                overallStats[file][1].append(y)\n",
    "            dots_x = []\n",
    "            dots_y = []\n",
    "            labels = []\n",
    "            for i in x_means:\n",
    "                dots_x.append(x_means[i][1])\n",
    "                dots_y.append(x_means[i][0])\n",
    "                labels.append(i)\n",
    "                ax[bp,bn].errorbar(x=x_means[i][1], y=x_means[i][0], label=i, fmt=markers[i])\n",
    "            if dataset == 'adult':\n",
    "                ax[bp,bn].set_xlim([-0.05,0.5])\n",
    "                ax[bp,bn].set_ylim([-0.05,0.5])\n",
    "            elif dataset == 'bank':\n",
    "                ax[bp,bn].set_xlim([-0.05,0.5])\n",
    "                ax[bp,bn].set_ylim([-0.05,0.5])\n",
    "            elif dataset == 'compas':\n",
    "                ax[bp,bn].set_xlim([-0.05,0.5])\n",
    "                ax[bp,bn].set_ylim([-0.05,0.5])\n",
    "            elif dataset == 'credit':\n",
    "                ax[bp,bn].set_xlim([-0.05,0.5])\n",
    "                ax[bp,bn].set_ylim([-0.05,0.5])\n",
    "            #ax[bp,bn].axvline(x=x_means['base'][1])\n",
    "            #ax[bp,bn].axhline(y=x_means['base'][0])\n",
    "            # D2 distribution lines\n",
    "            if metric == 'eod':\n",
    "                ax[bp,bn].axvline(x=(1.0 - base_results['undersample']['imbalance_1.0_1.0']['original'].accuracy()))\n",
    "                ax[bp,bn].axhline(y=abs(base_results['undersample']['imbalance_1.0_1.0']['original'].equal_opportunity_difference()))\n",
    "            elif metric == 'dp':\n",
    "                ax[bp,bn].axvline(x=(1.0 - base_results['undersample']['imbalance_1.0_1.0']['original'].accuracy()))\n",
    "                ax[bp,bn].axhline(y=abs(base_results['undersample']['imbalance_1.0_1.0']['original'].statistical_parity_difference()))\n",
    "            ax[bp,bn].set_xlabel('Error Rate')\n",
    "            ax[bp,bn].set_ylabel(axisTitles[metric])\n",
    "    plt.legend(bbox_to_anchor=(2.1,3.3))\n",
    "    plt.show()\n",
    "    # Ranking Plots\n",
    "    rankingMetric = 'max'\n",
    "    if rankingMetric == 'mean':\n",
    "        new_err = []\n",
    "        new_fairness = []\n",
    "        new_x = []\n",
    "        for i in overallStats:\n",
    "            new_x.append(i)\n",
    "            new_err.append((np.mean(overallStats[i][0]), np.var(overallStats[i][0])))\n",
    "            new_fairness.append((np.mean(overallStats[i][1]), np.var(overallStats[i][1])))\n",
    "        # Sort lists together (increasing order of error rate/fairness violation with respect to means)\n",
    "        # Error rate\n",
    "        error = {new_x[i]:new_err[i] for i in range(len(new_x))}\n",
    "        error_lst = {k: v for k, v in sorted(error.items(), key=lambda item: item[1][0])}\n",
    "        final_err = list(error_lst.values())\n",
    "        final_x = list(error_lst.keys())\n",
    "        plt.figure()\n",
    "        plt.errorbar(x=final_x, y=[i[0] for i in final_err], yerr=[i[1] for i in final_err], fmt='o')\n",
    "        plt.xticks(rotation=270)\n",
    "        plt.axhline(y=(1.0 - base_results['undersample']['imbalance_1.0_1.0']['original'].accuracy()))\n",
    "        plt.title('Error Rate Ranking')\n",
    "        plt.show()\n",
    "        # Fairness\n",
    "        error = {new_x[i]:new_fairness[i] for i in range(len(new_x))}\n",
    "        error_lst = {k: v for k, v in sorted(error.items(), key=lambda item: item[1][0])}\n",
    "        final_fairness = list(error_lst.values())\n",
    "        final_x = list(error_lst.keys())\n",
    "        plt.figure()\n",
    "        plt.errorbar(x=final_x, y=[i[0] for i in final_err], yerr=[i[1] for i in final_fairness], fmt='o')\n",
    "        plt.xticks(rotation=270)\n",
    "        if metric == 'eod':\n",
    "            plt.axhline(y=abs(base_results['undersample']['imbalance_1.0_1.0']['original'].equal_opportunity_difference()))\n",
    "        elif metric == 'dp':\n",
    "            plt.axhline(y=abs(base_results['undersample']['imbalance_1.0_1.0']['original'].statistical_parity_difference()))\n",
    "        plt.title('Fairness Ranking')\n",
    "        plt.show()\n",
    "    else:\n",
    "        new_err = []\n",
    "        new_fairness = []\n",
    "        new_x = []\n",
    "        for i in overallStats:\n",
    "            new_x.append(i)\n",
    "            new_err.append(np.max(overallStats[i][0]))\n",
    "            new_fairness.append(np.max(overallStats[i][1]))\n",
    "        # Error rate\n",
    "        error = {new_x[i]:new_err[i] for i in range(len(new_x))}\n",
    "        error_lst = {k: v for k, v in sorted(error.items(), key=lambda item: item[1])}\n",
    "        final_err = list(error_lst.values())\n",
    "        final_x = list(error_lst.keys())\n",
    "        plt.figure()\n",
    "        plt.errorbar(x=final_x, y=final_err, fmt='o')\n",
    "        plt.xticks(rotation=270)\n",
    "        plt.axhline(y=(1.0 - base_results['undersample']['imbalance_1.0_1.0']['original'].accuracy()))\n",
    "        plt.title('Error Rate Ranking')\n",
    "        plt.show()\n",
    "        # Fairness\n",
    "        error = {new_x[i]:new_fairness[i] for i in range(len(new_x))}\n",
    "        error_lst = {k: v for k, v in sorted(error.items(), key=lambda item: item[1])}\n",
    "        final_fairness = list(error_lst.values())\n",
    "        final_x = list(error_lst.keys())\n",
    "        plt.figure()\n",
    "        plt.errorbar(x=final_x, y=final_fairness, fmt='o')\n",
    "        plt.xticks(rotation=270)\n",
    "        if metric == 'eod':\n",
    "            plt.axhline(y=abs(base_results['undersample']['imbalance_1.0_1.0']['original'].equal_opportunity_difference()))\n",
    "        elif metric == 'dp':\n",
    "            plt.axhline(y=abs(base_results['undersample']['imbalance_1.0_1.0']['original'].statistical_parity_difference()))\n",
    "        plt.title('Fairness Ranking')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35f3918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#makeUndersamplePlots(files)\n",
    "#final_makeGridUndersamplePlots(files, metric='eod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4fd998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from data_utils import preprocessDataset\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "import copy\n",
    "from aif360.metrics import ClassificationMetric\n",
    "def getBayesOptimalResults(dim=10):\n",
    "    with open('/media/data_dump/Mohit/neurips2022_data/synthetic/means.pkl', 'rb') as f:\n",
    "        means = pickle.load(f)\n",
    "    _, test_dataset_original = preprocessDataset('/media/data_dump/Mohit/neurips2022_data/synthetic/raw/original_train.csv', '/media/data_dump/Mohit/neurips2022_data/synthetic/raw/original_test.csv', 'synthetic')\n",
    "    x = test_dataset_original.features[:,:-1]\n",
    "    z = test_dataset_original.protected_attributes.squeeze()\n",
    "    y = []\n",
    "    var = 1\n",
    "    #group_0_0_pdf = np.random.multivariate_normal(mean=means[0], cov=var*np.eye(1))\n",
    "    #group_0_1_pdf = np.random.multivariate_normal(mean=means[1], cov=var*np.eye(1))\n",
    "    #group_1_0_pdf = np.random.multivariate_normal(mean=means[2], cov=var*np.eye(1))\n",
    "    #group_1_1_pdf = np.random.multivariate_normal(mean=means[3], cov=var*np.eye(1))\n",
    "    for i in range(len(x)):\n",
    "        if z[i] == 0:\n",
    "            # first index y, second z\n",
    "            pdf_0_0 = multivariate_normal.pdf(x[i], mean=means[0], cov=var*np.eye(dim))\n",
    "            pdf_1_0 = multivariate_normal.pdf(x[i], mean=means[2], cov=var*np.eye(dim))\n",
    "            if pdf_0_0 > pdf_1_0:\n",
    "                y.append(0)\n",
    "            else:\n",
    "                y.append(1)\n",
    "        elif z[i] == 1:\n",
    "            # first index y, second z\n",
    "            pdf_0_1 = multivariate_normal.pdf(x[i], mean=means[1], cov=var*np.eye(dim))\n",
    "            pdf_1_1 = multivariate_normal.pdf(x[i], mean=means[3], cov=var*np.eye(dim))\n",
    "            if pdf_0_1 > pdf_1_1:\n",
    "                y.append(0)\n",
    "            else:\n",
    "                y.append(1)\n",
    "#     for i in range(len(x)):\n",
    "#         pdfs = [multivariate_normal.pdf(x[i], mean=means[0], cov=var*np.eye(dim)),\n",
    "#         multivariate_normal.pdf(x[i], mean=means[1], cov=var*np.eye(dim)),\n",
    "#         multivariate_normal.pdf(x[i], mean=means[2], cov=var*np.eye(dim)),\n",
    "#         multivariate_normal.pdf(x[i], mean=means[3], cov=var*np.eye(dim))]\n",
    "#         correctIndex = np.argmax(pdfs)\n",
    "#         if correctIndex == 0 or correctIndex == 1:\n",
    "#             y.append(0)\n",
    "#         elif correctIndex == 2 or correctIndex == 3:\n",
    "#             y.append(1)\n",
    "    pred_test_set = copy.deepcopy(test_dataset_original)\n",
    "    pred_test_set.labels = np.asarray(y)\n",
    "    privileged_groups = [{'sensitive':1}]\n",
    "    unprivileged_groups = [{'sensitive': 0}]\n",
    "    return ClassificationMetric(test_dataset_original, pred_test_set, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d61791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = getBayesOptimalResults(dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be40ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.accuracy(), abs(temp.statistical_parity_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947117fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def max_GridUndersamplePlots(files, metric='eod', testSet='orig', dataset=dataset, statType='max'):\n",
    "    if dataset == 'compas':\n",
    "        omitCls = ['meta_fair_fdr']\n",
    "    else:\n",
    "        omitCls = []\n",
    "    axisTitles = {\n",
    "        'dp': 'Statistical Parity Diff.',\n",
    "        'eod': 'Equal Oppor. Diff.'\n",
    "    }\n",
    "    plotLimits = {\n",
    "        'synthetic': ([0.05, 0.55], [-0.05, 0.35]),\n",
    "        'adult': ([-0.05, 0.55], [-0.05, 0.55]),\n",
    "        'bank': ([-0.05, 0.55], [-0.05, 0.55]),\n",
    "        'compas': ([-0.05, 0.55], [-0.05, 0.55]),\n",
    "        'credit': ([-0.05, 0.55], [-0.05, 0.55])\n",
    "    }\n",
    "    markers = {\n",
    "        'base': '+',\n",
    "        'jiang_nachum': 'o',\n",
    "        'rew':'o',\n",
    "        'fairgan':'o',\n",
    "        'exp_grad_eodds':'^',\n",
    "        'meta_fair_fdr':'^',\n",
    "        'prej_remover':'^',\n",
    "        'gerry_fair': '^',\n",
    "        'cal_eq':'s',\n",
    "        'eq':'s',\n",
    "        'reject':'s'\n",
    "    }\n",
    "    colors = {\n",
    "        'base': '#1f77b4',\n",
    "        'jiang_nachum': '#ff7f0e',\n",
    "        'rew':'#2ca02c',\n",
    "        'fairgan':'#d62728',\n",
    "        'exp_grad_eodds':'#9467bd',\n",
    "        'meta_fair_fdr':'#8c564b',\n",
    "        'prej_remover':'#e377c2',\n",
    "        'gerry_fair': '#7f7f7f',\n",
    "        'cal_eq':'#bcbd22',\n",
    "        'eq':'#17becf',\n",
    "        'reject':'#1f77b4'\n",
    "    }\n",
    "    fig, ax = plt.subplots(ncols=4)\n",
    "    fig.subplots_adjust(wspace=0.5)\n",
    "    fig.set_figheight(3)\n",
    "    fig.set_figwidth(12)\n",
    "    overallStats = {}\n",
    "    fixed_bp_variable_bn_stats = {}\n",
    "    fixed_bn_variable_bp_stats = {}\n",
    "    for file in files:\n",
    "        if file not in omitCls:\n",
    "            overallStats[file] = [[],[]]\n",
    "            fixed_bp_variable_bn_stats[file] = [[],[]]\n",
    "            fixed_bn_variable_bp_stats[file] = [[],[]]\n",
    "    for bp,beta_pos in enumerate(range(1,11)):\n",
    "        beta_pos /= 10\n",
    "        for bn,beta_neg in enumerate(range(1,11)):\n",
    "            beta_neg /= 10\n",
    "            # Calculating required Quantities\n",
    "            y_means = []\n",
    "            y_errs  = []\n",
    "            x_means = {}\n",
    "            for file in files:\n",
    "                if file in omitCls:\n",
    "                    continue\n",
    "                results = files[file]\n",
    "                base_results = files['base']\n",
    "                #x = sorted([float(i.split('_')[1]) for i in results['undersample'].keys()])\n",
    "                try:\n",
    "                    if metric == 'dp':\n",
    "                        y_bal = abs(results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['balanced'].statistical_parity_difference())\n",
    "                        y_biased = abs(results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['biased'].statistical_parity_difference())\n",
    "                        y_orig = abs(results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['original'].statistical_parity_difference())\n",
    "                    elif metric == 'eod':\n",
    "                        y_bal = abs(results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['balanced'].equal_opportunity_difference())\n",
    "                        y_biased = abs(results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['biased'].equal_opportunity_difference())\n",
    "                        y_orig = abs(results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['original'].equal_opportunity_difference())\n",
    "                    # Calculate accuracy\n",
    "                    x_err_rate_bal = (1 - abs(results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['balanced'].accuracy()))\n",
    "                    x_err_rate_biased = (1 - abs(results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['biased'].accuracy()))\n",
    "                    x_err_rate_orig = (1 - abs(results['undersample'][f'imbalance_{str(beta_pos)}_{str(beta_neg)}']['original'].accuracy()))\n",
    "                except TypeError as te:\n",
    "                    print(te)\n",
    "                    continue\n",
    "                if testSet == 'bal':\n",
    "                    y = y_bal\n",
    "                    x = x_err_rate_bal\n",
    "                elif testSet == 'biased':\n",
    "                    y = y_biased\n",
    "                    x = x_err_rate_biased\n",
    "                elif testSet == 'orig':\n",
    "                    y = y_orig\n",
    "                    x = x_err_rate_orig\n",
    "                x_means[file] = [y,x]\n",
    "                overallStats[file][0].append(x)\n",
    "                overallStats[file][1].append(y)\n",
    "                if beta_pos == 1.0:\n",
    "                    fixed_bp_variable_bn_stats[file][0].append(x)\n",
    "                    fixed_bp_variable_bn_stats[file][1].append(y)\n",
    "                if beta_neg == 1.0:\n",
    "                    fixed_bn_variable_bp_stats[file][0].append(x)\n",
    "                    fixed_bn_variable_bp_stats[file][1].append(y)\n",
    "    # Overall Mean\n",
    "    #min_x = 11\n",
    "    #min_y = 11\n",
    "    for i in overallStats:\n",
    "        if statType == 'max':\n",
    "            ax[1].errorbar(x=max(overallStats[i][0]), y=max(overallStats[i][1]), label=i, fmt=markers[i], color=colors[i])\n",
    "            #if dataset != 'synthetic':\n",
    "            #    ax[1].set_xlim([-0.05,0.5])\n",
    "            #    ax[1].set_ylim([-0.05,0.5])\n",
    "            #else:\n",
    "            #    ax[1].set_xlim([0.15,0.65])\n",
    "            #    ax[1].set_ylim([-0.1,1.05])\n",
    "            ax[1].set_ylim(plotLimits[dataset][1])\n",
    "            ax[1].set_xlim(plotLimits[dataset][0])\n",
    "            #if max(overallStats[i][1]) < min_y:\n",
    "            #    min_y = max(overallStats[i][1])\n",
    "            #    min_x = max(overallStats[i][0])\n",
    "        else:\n",
    "            ax[1].errorbar(x=np.mean(overallStats[i][0]), y=np.mean(overallStats[i][1]), xerr=np.std(overallStats[i][0]), yerr=np.std(overallStats[i][1]), label=i, fmt=markers[i], color=colors[i])\n",
    "            #ax[0].errorbar(x=np.mean(overallStats[i][0]), y=np.mean(overallStats[i][1]), label=i, fmt=markers[i], color=colors[i])\n",
    "            #if dataset != 'synthetic':\n",
    "            #    ax[1].set_xlim([-0.05,0.5])\n",
    "            #    ax[1].set_ylim([-0.05,0.5])\n",
    "            #elif dataset == 'synthetic':\n",
    "            #    #ax[1].set_xlim([0.15,0.65])\n",
    "            #    #ax[1].set_ylim([-0.1,1.05])\n",
    "            ax[1].set_ylim(plotLimits[dataset][1])\n",
    "            ax[1].set_xlim(plotLimits[dataset][0])\n",
    "    # D2 distribution lines\n",
    "    if dataset != 'synthetic':\n",
    "        if metric == 'eod':\n",
    "            ax[1].axvline(x=(1.0 - base_results['undersample']['imbalance_1.0_1.0']['original'].accuracy()))\n",
    "            ax[1].axhline(y=abs(base_results['undersample']['imbalance_1.0_1.0']['original'].equal_opportunity_difference()))\n",
    "        elif metric == 'dp':\n",
    "            ax[1].axvline(x=(1.0 - base_results['undersample']['imbalance_1.0_1.0']['original'].accuracy()))\n",
    "            ax[1].axhline(y=abs(base_results['undersample']['imbalance_1.0_1.0']['original'].statistical_parity_difference()))\n",
    "        ax[1].set_xlabel('Error Rate')\n",
    "        ax[1].set_ylabel(axisTitles[metric])\n",
    "        ax[1].set_title('Mean Over all settings', fontsize=10)\n",
    "    else:\n",
    "        bayes_optimal_results = getBayesOptimalResults()\n",
    "        if metric == 'eod':\n",
    "            ax[1].axvline(x=(1.0 - bayes_optimal_results.accuracy()))\n",
    "            ax[1].axhline(y=abs(bayes_optimal_results.equal_opportunity_difference()))\n",
    "        elif metric == 'dp':\n",
    "            ax[1].axvline(x=(1.0 - bayes_optimal_results.accuracy()))\n",
    "            ax[1].axhline(y=abs(bayes_optimal_results.statistical_parity_difference()))\n",
    "        ax[1].set_xlabel('Error Rate')\n",
    "        ax[1].set_ylabel(axisTitles[metric])\n",
    "        ax[1].set_title('Mean Over all settings', fontsize=10)\n",
    "\n",
    "    # BP fixed, BN variable\n",
    "    #min_x = 11\n",
    "    #min_y = 11\n",
    "    for i in fixed_bp_variable_bn_stats:\n",
    "        if statType == 'max':\n",
    "            ax[2].errorbar(x=max(fixed_bp_variable_bn_stats[i][0]), y=max(fixed_bp_variable_bn_stats[i][1]), label=i, fmt=markers[i], color=colors[i])\n",
    "        else:\n",
    "            ax[2].errorbar(x=np.mean(fixed_bp_variable_bn_stats[i][0]), xerr=np.std(fixed_bp_variable_bn_stats[i][0]), y=np.mean(fixed_bp_variable_bn_stats[i][1]), yerr=np.std(fixed_bp_variable_bn_stats[i][1]), label=i, fmt=markers[i], color=colors[i])\n",
    "            #ax[1].errorbar(x=np.mean(fixed_bp_variable_bn_stats[i][0]), y=np.mean(fixed_bp_variable_bn_stats[i][1]), label=i, fmt=markers[i], color=colors[i])\n",
    "        #if dataset != 'synthetic':\n",
    "        #    ax[2].set_xlim([-0.05,0.5])\n",
    "        #    ax[2].set_ylim([-0.05,0.5])\n",
    "        #else:\n",
    "        #    ax[2].set_xlim([0.15,0.65])\n",
    "        #    ax[2].set_ylim([-0.1,1.05])\n",
    "        ax[2].set_ylim(plotLimits[dataset][1])\n",
    "        ax[2].set_xlim(plotLimits[dataset][0])\n",
    "        #if max(fixed_bp_variable_bn_stats[i][1]) < min_y:\n",
    "        #    min_y = max(fixed_bp_variable_bn_stats[i][1])\n",
    "        #    min_x = max(fixed_bp_variable_bn_stats[i][0])\n",
    "    # D2 distribution lines\n",
    "    if dataset != 'synthetic':\n",
    "        if metric == 'eod':\n",
    "            ax[2].axvline(x=(1.0 - base_results['undersample']['imbalance_1.0_1.0']['original'].accuracy()))\n",
    "            ax[2].axhline(y=abs(base_results['undersample']['imbalance_1.0_1.0']['original'].equal_opportunity_difference()))\n",
    "        elif metric == 'dp':\n",
    "            ax[2].axvline(x=(1.0 - base_results['undersample']['imbalance_1.0_1.0']['original'].accuracy()))\n",
    "            ax[2].axhline(y=abs(base_results['undersample']['imbalance_1.0_1.0']['original'].statistical_parity_difference()))\n",
    "        ax[2].set_xlabel('Error Rate')\n",
    "        ax[2].set_ylabel(axisTitles[metric])\n",
    "        ax[2].set_title(r'Mean over all $β_{p}$=1.0', fontsize=10)\n",
    "    else:\n",
    "        bayes_optimal_results = getBayesOptimalResults()\n",
    "        if metric == 'eod':\n",
    "            ax[2].axvline(x=(1.0 - bayes_optimal_results.accuracy()))\n",
    "            ax[2].axhline(y=abs(bayes_optimal_results.equal_opportunity_difference()))\n",
    "        elif metric == 'dp':\n",
    "            ax[2].axvline(x=(1.0 - bayes_optimal_results.accuracy()))\n",
    "            ax[2].axhline(y=abs(bayes_optimal_results.statistical_parity_difference()))\n",
    "        ax[2].set_xlabel('Error Rate')\n",
    "        ax[2].set_ylabel(axisTitles[metric])\n",
    "        ax[2].set_title(r'Mean over all $β_{p}$=1.0', fontsize=10)\n",
    "    \n",
    "    # BN fixed, BP variable\n",
    "    #min_x = 11\n",
    "    #min_y = 11\n",
    "    for i in fixed_bp_variable_bn_stats:\n",
    "        if statType == 'max':\n",
    "            ax[3].errorbar(x=max(fixed_bn_variable_bp_stats[i][0]), y=max(fixed_bn_variable_bp_stats[i][1]), label=i, fmt=markers[i], color=colors[i])\n",
    "        else:\n",
    "            ax[3].errorbar(x=np.mean(fixed_bn_variable_bp_stats[i][0]), xerr=np.std(fixed_bn_variable_bp_stats[i][0]), y=np.mean(fixed_bn_variable_bp_stats[i][1]), yerr=np.mean(fixed_bn_variable_bp_stats[i][1]), label=i, fmt=markers[i], color=colors[i])\n",
    "            #ax[2].errorbar(x=np.mean(fixed_bn_variable_bp_stats[i][0]), y=np.mean(fixed_bn_variable_bp_stats[i][1]), label=i, fmt=markers[i], color=colors[i])\n",
    "        #if dataset != 'synthetic':\n",
    "        #    ax[3].set_xlim([-0.05,0.5])\n",
    "        #    ax[3].set_ylim([-0.05,0.5])\n",
    "        #else:\n",
    "        #    ax[3].set_xlim([0.15,0.65])\n",
    "        #    ax[3].set_ylim([-0.1,1.05])\n",
    "        ax[3].set_ylim(plotLimits[dataset][1])\n",
    "        ax[3].set_xlim(plotLimits[dataset][0])\n",
    "        #if max(fixed_bn_variable_bp_stats[i][1]) < min_y:\n",
    "        #    min_y = max(fixed_bn_variable_bp_stats[i][1])\n",
    "        #    min_x = max(fixed_bn_variable_bp_stats[i][0])\n",
    "    # D2 distribution lines\n",
    "    if dataset != 'synthetic':\n",
    "        if metric == 'eod':\n",
    "            ax[3].axvline(x=(1.0 - base_results['undersample']['imbalance_1.0_1.0']['original'].accuracy()))\n",
    "            ax[3].axhline(y=abs(base_results['undersample']['imbalance_1.0_1.0']['original'].equal_opportunity_difference()))\n",
    "        elif metric == 'dp':\n",
    "            ax[3].axvline(x=(1.0 - base_results['undersample']['imbalance_1.0_1.0']['original'].accuracy()))\n",
    "            ax[3].axhline(y=abs(base_results['undersample']['imbalance_1.0_1.0']['original'].statistical_parity_difference()))\n",
    "        ax[3].set_xlabel('Error Rate')\n",
    "        ax[3].set_ylabel(axisTitles[metric])\n",
    "        ax[3].set_title(r'Mean over all $β_{n}$=1.0', fontsize=10)\n",
    "    else:\n",
    "        bayes_optimal_results = getBayesOptimalResults()\n",
    "        if metric == 'eod':\n",
    "            ax[3].axvline(x=(1.0 - bayes_optimal_results.accuracy()))\n",
    "            ax[3].axhline(y=abs(bayes_optimal_results.equal_opportunity_difference()))\n",
    "        elif metric == 'dp':\n",
    "            ax[3].axvline(x=(1.0 - bayes_optimal_results.accuracy()))\n",
    "            ax[3].axhline(y=abs(bayes_optimal_results.statistical_parity_difference()))\n",
    "        ax[3].set_xlabel('Error Rate')\n",
    "        ax[3].set_ylabel(axisTitles[metric])\n",
    "        ax[3].set_title(r'Mean over all $β_{n}$=1.0', fontsize=10)\n",
    "    \n",
    "    # Frequency Plot\n",
    "    cls_freqs = {}\n",
    "    for i in files:\n",
    "        cls_freqs[i] = 0\n",
    "    del cls_freqs['base']\n",
    "    for beta_pos in range(1,11):\n",
    "        beta_pos /= 10\n",
    "        for beta_neg in range(1,11):\n",
    "            beta_neg /=10\n",
    "            current_coord = {}\n",
    "            for file in files:\n",
    "                try:\n",
    "                    if metric == 'eod':\n",
    "                        current_coord[file] = (1 - files[file]['undersample'][f'imbalance_{beta_pos}_{beta_neg}']['original'].accuracy(), abs(files[file]['undersample'][f'imbalance_{beta_pos}_{beta_neg}']['original'].equal_opportunity_difference()))\n",
    "                    elif metric == 'dp':\n",
    "                        current_coord[file] = (1 - files[file]['undersample'][f'imbalance_{beta_pos}_{beta_neg}']['original'].accuracy(), abs(files[file]['undersample'][f'imbalance_{beta_pos}_{beta_neg}']['original'].statistical_parity_difference()))\n",
    "                except TypeError as te:\n",
    "                    #print(te)\n",
    "                    continue\n",
    "            del current_coord['base']\n",
    "            sorted_acc = sorted(list(current_coord.items()), key=lambda x: x[1][0])\n",
    "            sorted_fairness = sorted(list(current_coord.items()), key=lambda x: x[1][1])\n",
    "            k = 5\n",
    "            top_k_classifiers = list(set([i[0] for i in sorted_acc][:k]).intersection(set([i[0] for i in sorted_fairness][:k])))\n",
    "            for i in current_coord:\n",
    "                if current_coord[i][0] < 0.5 and current_coord[i][1] < 0.5:\n",
    "                    flag = 0\n",
    "                    for j in current_coord:\n",
    "                        if j != i:\n",
    "                            # Check whether a point exists\n",
    "                            if current_coord[j][0] < current_coord[i][0] and current_coord[j][1] < current_coord[i][1]:\n",
    "                                flag = 1\n",
    "                                break\n",
    "                    if flag == 0 and i in top_k_classifiers:\n",
    "                        cls_freqs[i] += 1\n",
    "    \n",
    "    # Relative Error-Fairness plots\n",
    "    relative_error_rates = {}\n",
    "    relative_fairness = {}\n",
    "    for i in cls_freqs:\n",
    "        relative_error_rates[i] = []\n",
    "        relative_fairness[i] = []\n",
    "    relative_error_rates['base'] = []\n",
    "    relative_fairness['base'] = []\n",
    "    for beta_pos in range(1,11):\n",
    "        beta_pos /= 10\n",
    "        for beta_neg in range(1,11):\n",
    "            beta_neg /= 10\n",
    "            for file in files:\n",
    "                try:\n",
    "                    if dataset != 'synthetic':\n",
    "                        if metric == 'eod':\n",
    "                            relative_error_rates[file].append((1 - files[file]['undersample'][f'imbalance_{beta_pos}_{beta_neg}']['original'].accuracy()) - (1 - files[file]['undersample'][f'imbalance_1.0_1.0']['original'].accuracy()))\n",
    "                            relative_fairness[file].append(abs(files[file]['undersample'][f'imbalance_{beta_pos}_{beta_neg}']['original'].equal_opportunity_difference()) - abs(files[file]['undersample'][f'imbalance_1.0_1.0']['original'].equal_opportunity_difference()))\n",
    "                        elif metric == 'dp':\n",
    "                            relative_error_rates[file].append((1 - files[file]['undersample'][f'imbalance_{beta_pos}_{beta_neg}']['original'].accuracy()) - (1 - files[file]['undersample'][f'imbalance_1.0_1.0']['original'].accuracy()))\n",
    "                            relative_fairness[file].append(abs(files[file]['undersample'][f'imbalance_{beta_pos}_{beta_neg}']['original'].equal_opportunity_difference()) - abs(files[file]['undersample'][f'imbalance_1.0_1.0']['original'].statistical_parity_difference()))\n",
    "                    else:\n",
    "                        if metric == 'eod':\n",
    "                            relative_error_rates[file].append((1 - files[file]['undersample'][f'imbalance_{beta_pos}_{beta_neg}']['original'].accuracy()) - (1 - bayes_optimal_results.accuracy()))\n",
    "                            relative_fairness[file].append(abs(files[file]['undersample'][f'imbalance_{beta_pos}_{beta_neg}']['original'].equal_opportunity_difference()) - abs(bayes_optimal_results.equal_opportunity_difference()))\n",
    "                        elif metric == 'dp':\n",
    "                            relative_error_rates[file].append((1 - files[file]['undersample'][f'imbalance_{beta_pos}_{beta_neg}']['original'].accuracy()) - (1 - bayes_optimal_results.accuracy()))\n",
    "                            relative_fairness[file].append(abs(files[file]['undersample'][f'imbalance_{beta_pos}_{beta_neg}']['original'].equal_opportunity_difference()) - abs(bayes_optimal_results.statistical_parity_difference()))\n",
    "                except TypeError as te:\n",
    "                    #print(te)\n",
    "                    continue\n",
    "    \"\"\"for i in relative_error_rates:\n",
    "        if i not in omitCls:\n",
    "            #ax[3].errorbar(x=max(relative_error_rates[i]), y=max(relative_fairness[i]), label=i, fmt=markers[i], color=colors[i])\n",
    "            ax[3].errorbar(x=np.mean(relative_error_rates[i]), y=np.mean(relative_fairness[i]),  xerr=np.var(relative_error_rates[i]), yerr=np.var(relative_fairness[i]), label=i, fmt=markers[i], color=colors[i])\n",
    "            #ax[1].set_xlim([-0.05,0.5])\n",
    "            #ax[1].set_ylim([-0.05,0.5])\n",
    "    ax[3].set_xlabel('Relative Error Rate')\n",
    "    ax[3].set_ylabel(axisTitles[metric])\n",
    "    ax[3].set_title('Overall Mean - Unbiased Perf.', fontsize=9)\"\"\"\n",
    "    for i in files:\n",
    "        if i not in omitCls:\n",
    "            #ax[3].errorbar(x=max(relative_error_rates[i]), y=max(relative_fairness[i]), label=i, fmt=markers[i], color=colors[i])\n",
    "            if metric == 'eod':\n",
    "                ax[0].errorbar(x = 1 - files[i]['undersample'][f'imbalance_1.0_1.0']['original'].accuracy(), y=abs(files[i]['undersample'][f'imbalance_1.0_1.0']['original'].equal_opportunity_difference()), label=i, fmt=markers[i], color=colors[i])\n",
    "            elif metric == 'dp':\n",
    "                ax[0].errorbar(x = 1 - files[i]['undersample'][f'imbalance_1.0_1.0']['original'].accuracy(), y=abs(files[i]['undersample'][f'imbalance_1.0_1.0']['original'].statistical_parity_difference()), label=i, fmt=markers[i], color=colors[i])\n",
    "    ax[0].set_xlabel('Error Rate')\n",
    "    ax[0].set_ylabel(axisTitles[metric])\n",
    "    ax[0].set_title('Results on Original Split', fontsize=10)\n",
    "    ax[0].set_ylim(plotLimits[dataset][1])\n",
    "    ax[0].set_xlim(plotLimits[dataset][0])\n",
    "    \n",
    "    if dataset == 'credit':\n",
    "        plt.legend(bbox_to_anchor=(1.1,1.1))\n",
    "    elif dataset == 'adult':\n",
    "        plt.legend(loc='right', bbox_to_anchor=(2.1,0.5))\n",
    "    elif dataset == 'compas':\n",
    "        plt.legend(loc='right', bbox_to_anchor=(2,0.5))\n",
    "    elif dataset == 'bank':\n",
    "        plt.legend(loc='right', bbox_to_anchor=(2,0.5))\n",
    "    elif dataset == 'synthetic':\n",
    "        plt.legend(loc='right', bbox_to_anchor=(2.1,0.5))\n",
    "    plt.show()\n",
    "    \n",
    "    fig.savefig(f'{dataset}_underRepresentation.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.bar(range(len(cls_freqs)), cls_freqs.values(), tick_label=list(cls_freqs.keys()))\n",
    "    plt.xticks(rotation=90, fontsize=8)\n",
    "    plt.title('Pareto Optimality Frequency', fontsize=9)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70076d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_GridUndersamplePlots(files, statType='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84111353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def makeLabelBiasPlots(files, labelBiasFactors=[0.0, 0.3, 0.7, 0.9], metrics=['dp', 'eod'], testSet='orig', dataset=dataset):\n",
    "#     axisTitles = {\n",
    "#         'dp': 'Statistical Parity Diff.',\n",
    "#         'eod': 'Equal Oppor. Diff.'\n",
    "#     }\n",
    "#     if dataset == 'compas':\n",
    "#         omitCls = ['meta_fair_fdr']\n",
    "#     else:\n",
    "#         omitCls = []\n",
    "#     fig, ax = plt.subplots(nrows=len(metrics), ncols=len(labelBiasFactors))\n",
    "#     fig.subplots_adjust(hspace=0.5, wspace=0.4)\n",
    "#     fig.set_figheight(5)\n",
    "#     fig.set_figwidth(15)\n",
    "#     for m,metric in enumerate(metrics):\n",
    "#         for l,label_bias in enumerate(labelBiasFactors):\n",
    "#             ax[m,l].title.set_text(f'Metric: {metric}, Label_Bias: {label_bias}')\n",
    "#             # Calculating required Quantities\n",
    "#             y_means = []\n",
    "#             y_errs  = []\n",
    "#             x_means = {}\n",
    "#             for file in files:\n",
    "#                 if file in omitCls:\n",
    "#                     continue\n",
    "#                 results = files[file]\n",
    "#                 if metric == 'dp':\n",
    "#                     y_bal = abs(results['label_bias'][f'imbalance_{str(label_bias)}']['balanced'].statistical_parity_difference())\n",
    "#                     y_biased = abs(results['label_bias'][f'imbalance_{str(label_bias)}']['biased'].statistical_parity_difference())\n",
    "#                     y_orig = abs(results['label_bias'][f'imbalance_{str(label_bias)}']['original'].statistical_parity_difference())\n",
    "#                 elif metric == 'eod':\n",
    "#                     y_bal = abs(results['label_bias'][f'imbalance_{str(label_bias)}']['balanced'].equal_opportunity_difference())\n",
    "#                     y_biased = abs(results['label_bias'][f'imbalance_{str(label_bias)}']['biased'].equal_opportunity_difference())\n",
    "#                     y_orig = abs(results['label_bias'][f'imbalance_{str(label_bias)}']['original'].equal_opportunity_difference())\n",
    "#                 # Calculate accuracy\n",
    "#                 x_err_rate_bal = 1 - abs(results['label_bias'][f'imbalance_{str(label_bias)}']['balanced'].accuracy())\n",
    "#                 x_err_rate_biased = 1 - abs(results['label_bias'][f'imbalance_{str(label_bias)}']['biased'].accuracy())\n",
    "#                 x_err_rate_orig = 1 - abs(results['label_bias'][f'imbalance_{str(label_bias)}']['original'].accuracy())\n",
    "#                 if testSet == 'bal':\n",
    "#                     y = y_bal\n",
    "#                     x = x_err_rate_bal\n",
    "#                 elif testSet == 'biased':\n",
    "#                     y = y_biased\n",
    "#                     x = x_err_rate_biased\n",
    "#                 elif testSet == 'orig':\n",
    "#                     y = y_orig\n",
    "#                     x = x_err_rate_orig\n",
    "#                 x_means[file] = [y,x]\n",
    "#             dots_x = []\n",
    "#             dots_y = []\n",
    "#             labels = []\n",
    "#             for i in x_means:\n",
    "#                 dots_x.append(x_means[i][1])\n",
    "#                 dots_y.append(x_means[i][0])\n",
    "#                 labels.append(i)\n",
    "#                 ax[m,l].errorbar(x=x_means[i][1], y=x_means[i][0], label=i, fmt='o')\n",
    "#             ax[m,l].axvline(x=x_means['base'][1])\n",
    "#             ax[m,l].axhline(y=x_means['base'][0])\n",
    "#             if dataset == 'adult':\n",
    "#                 ax[m,l].set_xlim([-0.1,0.5])\n",
    "#                 ax[m,l].set_ylim([-0.1,1.1])\n",
    "#             elif dataset == 'bank':\n",
    "#                 ax[m,l].set_xlim([-0.1,1])\n",
    "#                 ax[m,l].set_ylim([-0.5,1.1])\n",
    "#             elif dataset == 'compas':\n",
    "#                 ax[m,l].set_xlim([-0.1,0.8])\n",
    "#                 ax[m,l].set_ylim([-0.5,1])\n",
    "#             ax[m,l].set_xlabel('Error Rate')\n",
    "#             ax[m,l].set_ylabel(axisTitles[metric])\n",
    "#     plt.xlabel('Error Rate')\n",
    "#     plt.legend(bbox_to_anchor=(1.9,1.8))\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Frequency Plot\n",
    "#     cls_freqs = {}\n",
    "#     for i in files:\n",
    "#         cls_freqs[i] = 0\n",
    "#     del cls_freqs['base']\n",
    "#     for lb in range(0,10):\n",
    "#         lb /= 10\n",
    "#         current_coord = {}\n",
    "#         for file in files:\n",
    "#             try:\n",
    "#                 if metric == 'eod':\n",
    "#                     current_coord[file] = (1 - files[file]['label_bias'][f'imbalance_{lb}']['original'].accuracy(), abs(files[file]['label_bias'][f'imbalance_{lb}']['original'].equal_opportunity_difference()))\n",
    "#                 elif metric == 'dp':\n",
    "#                     current_coord[file] = (1 - files[file]['label_bias'][f'imbalance_{lb}']['original'].accuracy(), abs(files[file]['label_bias'][f'imbalance_{lb}']['original'].statistical_parity_difference()))\n",
    "#             except TypeError as te:\n",
    "#                 #print(te)\n",
    "#                 continue\n",
    "#         del current_coord['base']\n",
    "#         sorted_acc = sorted(list(current_coord.items()), key=lambda x: x[1][0])\n",
    "#         sorted_fairness = sorted(list(current_coord.items()), key=lambda x: x[1][1])\n",
    "#         k = 5\n",
    "#         top_k_classifiers = list(set([i[0] for i in sorted_acc][:k]).intersection(set([i[0] for i in sorted_fairness][:k])))\n",
    "#         for i in current_coord:\n",
    "#             if current_coord[i][0] < 0.5 and current_coord[i][1] < 0.5:\n",
    "#                 flag = 0\n",
    "#                 for j in current_coord:\n",
    "#                     if j != i:\n",
    "#                         # Check whether a point exists\n",
    "#                         if current_coord[j][0] < current_coord[i][0] and current_coord[j][1] < current_coord[i][1]:\n",
    "#                             flag = 1\n",
    "#                             break\n",
    "#                 if flag == 0 and i in top_k_classifiers:\n",
    "#                     cls_freqs[i] += 1\n",
    "#     plt.figure()\n",
    "#     plt.bar(range(len(cls_freqs)), cls_freqs.values(), tick_label=list(cls_freqs.keys()))\n",
    "#     plt.xticks(rotation=90, fontsize=8)\n",
    "#     plt.title('Pareto Dominance Frequency', fontsize=9)\n",
    "#     plt.show()\n",
    "import math\n",
    "def max_labelBiasPlots(files, metric='eod', testSet='orig', dataset=dataset, statType='max'):\n",
    "    if dataset == 'compas':\n",
    "        omitCls = ['meta_fair_fdr']\n",
    "    else:\n",
    "        omitCls = []\n",
    "    axisTitles = {\n",
    "        'dp': 'Statistical Parity Diff.',\n",
    "        'eod': 'Equal Oppor. Diff.'\n",
    "    }\n",
    "    markers = {\n",
    "        'base': '+',\n",
    "        'jiang_nachum': 'o',\n",
    "        'rew':'o',\n",
    "        'fairgan':'o',\n",
    "        'exp_grad_eodds':'^',\n",
    "        'meta_fair_fdr':'^',\n",
    "        'prej_remover':'^',\n",
    "        'gerry_fair': '^',\n",
    "        'cal_eq':'s',\n",
    "        'eq':'s',\n",
    "        'reject':'s'\n",
    "    }\n",
    "    colors = {\n",
    "        'base': '#1f77b4',\n",
    "        'jiang_nachum': '#ff7f0e',\n",
    "        'rew':'#2ca02c',\n",
    "        'fairgan':'#d62728',\n",
    "        'exp_grad_eodds':'#9467bd',\n",
    "        'meta_fair_fdr':'#8c564b',\n",
    "        'prej_remover':'#e377c2',\n",
    "        'gerry_fair': '#7f7f7f',\n",
    "        'cal_eq':'#bcbd22',\n",
    "        'eq':'#17becf',\n",
    "        'reject':'#1f77b4'\n",
    "    }\n",
    "    fig, ax = plt.subplots(ncols=2)\n",
    "    fig.subplots_adjust(wspace=0.3)\n",
    "    fig.set_figheight(3)\n",
    "    fig.set_figwidth(8)\n",
    "    overallStats = {}\n",
    "    for file in files:\n",
    "        if file not in omitCls:\n",
    "            overallStats[file] = [[],[]]\n",
    "    for lb,label_bias in enumerate(range(0,10)):\n",
    "        label_bias /= 10\n",
    "        # Calculating required Quantities\n",
    "        y_means = []\n",
    "        y_errs  = []\n",
    "        x_means = {}\n",
    "        for file in files:\n",
    "            if file in omitCls:\n",
    "                continue\n",
    "            results = files[file]\n",
    "            base_results = files['base']\n",
    "            #x = sorted([float(i.split('_')[1]) for i in results['undersample'].keys()])\n",
    "            try:\n",
    "                if metric == 'dp':\n",
    "                    y_bal = abs(results['label_bias'][f'imbalance_{str(label_bias)}']['balanced'].statistical_parity_difference())\n",
    "                    y_biased = abs(results['label_bias'][f'imbalance_{str(label_bias)}']['biased'].statistical_parity_difference())\n",
    "                    y_orig = abs(results['label_bias'][f'imbalance_{str(label_bias)}']['original'].statistical_parity_difference())\n",
    "                elif metric == 'eod':\n",
    "                    y_bal = abs(results['label_bias'][f'imbalance_{str(label_bias)}']['balanced'].equal_opportunity_difference())\n",
    "                    y_biased = abs(results['label_bias'][f'imbalance_{str(label_bias)}']['biased'].equal_opportunity_difference())\n",
    "                    y_orig = abs(results['label_bias'][f'imbalance_{str(label_bias)}']['original'].equal_opportunity_difference())\n",
    "                # Calculate accuracy\n",
    "                x_err_rate_bal = (1 - abs(results['label_bias'][f'imbalance_{str(label_bias)}']['balanced'].accuracy()))\n",
    "                x_err_rate_biased = (1 - abs(results['label_bias'][f'imbalance_{str(label_bias)}']['biased'].accuracy()))\n",
    "                x_err_rate_orig = (1 - abs(results['label_bias'][f'imbalance_{str(label_bias)}']['original'].accuracy()))\n",
    "            except TypeError as te:\n",
    "                continue\n",
    "            if testSet == 'bal':\n",
    "                y = y_bal\n",
    "                x = x_err_rate_bal\n",
    "            elif testSet == 'biased':\n",
    "                y = y_biased\n",
    "                x = x_err_rate_biased\n",
    "            elif testSet == 'orig':\n",
    "                y = y_orig\n",
    "                x = x_err_rate_orig\n",
    "            x_means[file] = [y,x]\n",
    "            overallStats[file][0].append(x)\n",
    "            overallStats[file][1].append(y)\n",
    "    # Overall Max\n",
    "    #min_x = 11\n",
    "    #min_y = 11\n",
    "    for i in overallStats:\n",
    "        if statType == 'max':\n",
    "            ax[0].errorbar(x=max(overallStats[i][0]), y=max(overallStats[i][1]), label=i, fmt=markers[i], color=colors[i])\n",
    "            if dataset != 'synthetic':\n",
    "                ax[0].set_xlim([-0.05,0.5])\n",
    "                ax[0].set_ylim([-0.05,0.5])\n",
    "            else:\n",
    "                ax[0].set_xlim([-0.05,0.7])\n",
    "                ax[0].set_ylim([-0.1,1.05])\n",
    "            #if max(overallStats[i][1]) < min_y:\n",
    "            #    min_y = max(overallStats[i][1])\n",
    "            #    min_x = max(overallStats[i][0])\n",
    "        else:\n",
    "            #ax[0].errorbar(x=np.mean(overallStats[i][0]), y=np.mean(overallStats[i][1]), xerr=np.std(overallStats[i][0]), yerr=np.std(overallStats[i][1]), label=i, fmt=markers[i], color=colors[i])\n",
    "            ax[0].errorbar(x=np.mean(overallStats[i][0]), y=np.mean(overallStats[i][1]), label=i, fmt=markers[i], color=colors[i])\n",
    "            if dataset != 'synthetic':\n",
    "                ax[0].set_xlim([-0.05,0.5])\n",
    "                ax[0].set_ylim([-0.05,0.5])\n",
    "            else:\n",
    "                ax[0].set_xlim([-0.05,0.7])\n",
    "                ax[0].set_ylim([-0.1,1.05])\n",
    "    # D2 distribution lines\n",
    "    if dataset != 'synthetic':\n",
    "        if metric == 'eod':\n",
    "            ax[0].axvline(x=(1.0 - base_results['label_bias']['imbalance_0.0']['original'].accuracy()))\n",
    "            ax[0].axhline(y=abs(base_results['label_bias']['imbalance_0.0']['original'].equal_opportunity_difference()))\n",
    "        elif metric == 'dp':\n",
    "            ax[0].axvline(x=(1.0 - base_results['label_bias']['imbalance_0.0']['original'].accuracy()))\n",
    "            ax[0].axhline(y=abs(base_results['label_bias']['imbalance_0.0']['original'].statistical_parity_difference()))\n",
    "        ax[0].set_xlabel('Error Rate')\n",
    "        ax[0].set_ylabel(axisTitles[metric])\n",
    "        ax[0].set_title('Overall Max', fontsize=9)\n",
    "    else:\n",
    "        bayes_optimal_results = getBayesOptimalResults()\n",
    "        if metric == 'eod':\n",
    "            ax[0].axvline(x=(1.0 - bayes_optimal_results.accuracy()))\n",
    "            ax[0].axhline(y=abs(bayes_optimal_results.equal_opportunity_difference()))\n",
    "        elif metric == 'dp':\n",
    "            ax[0].axvline(x=(1.0 - bayes_optimal_results.accuracy()))\n",
    "            ax[0].axhline(y=abs(bayes_optimal_results.statistical_parity_difference()))\n",
    "        ax[0].set_xlabel('Error Rate')\n",
    "        ax[0].set_ylabel(axisTitles[metric])\n",
    "        ax[0].set_title('Overall Max.', fontsize=9)\n",
    "    \n",
    "    # Frequency Plot\n",
    "    cls_freqs = {}\n",
    "    for i in files:\n",
    "        cls_freqs[i] = 0\n",
    "    del cls_freqs['base']\n",
    "    for label_bias in range(0,10):\n",
    "        label_bias /= 10\n",
    "        current_coord = {}\n",
    "        for file in files:\n",
    "            try:\n",
    "                if metric == 'eod':\n",
    "                    current_coord[file] = (1 - files[file]['label_bias'][f'imbalance_{label_bias}']['original'].accuracy(), abs(files[file]['label_bias'][f'imbalance_{label_bias}']['original'].equal_opportunity_difference()))\n",
    "                elif metric == 'dp':\n",
    "                    current_coord[file] = (1 - files[file]['label_bias'][f'imbalance_{label_bias}']['original'].accuracy(), abs(files[file]['label_bias'][f'imbalance_{label_bias}']['original'].statistical_parity_difference()))\n",
    "            except TypeError as te:\n",
    "                #print(te)\n",
    "                continue\n",
    "        del current_coord['base']\n",
    "        sorted_acc = sorted(list(current_coord.items()), key=lambda x: x[1][0])\n",
    "        sorted_fairness = sorted(list(current_coord.items()), key=lambda x: x[1][1])\n",
    "        k = 5\n",
    "        top_k_classifiers = list(set([i[0] for i in sorted_acc][:k]).intersection(set([i[0] for i in sorted_fairness][:k])))\n",
    "        for i in current_coord:\n",
    "            if current_coord[i][0] < 0.5 and current_coord[i][1] < 0.5:\n",
    "                flag = 0\n",
    "                for j in current_coord:\n",
    "                    if j != i:\n",
    "                        # Check whether a point exists\n",
    "                        if current_coord[j][0] < current_coord[i][0] and current_coord[j][1] < current_coord[i][1]:\n",
    "                            flag = 1\n",
    "                            break\n",
    "                if flag == 0 and i in top_k_classifiers:\n",
    "                    cls_freqs[i] += 1\n",
    "    \n",
    "    # Relative Error-Fairness plots\n",
    "    relative_error_rates = {}\n",
    "    relative_fairness = {}\n",
    "    for i in cls_freqs:\n",
    "        relative_error_rates[i] = []\n",
    "        relative_fairness[i] = []\n",
    "    relative_error_rates['base'] = []\n",
    "    relative_fairness['base'] = []\n",
    "    for label_bias in range(0,10):\n",
    "        label_bias /= 10\n",
    "        for file in files:\n",
    "            try:\n",
    "                if dataset != 'synthetic':\n",
    "                    if metric == 'eod':\n",
    "                        relative_error_rates[file].append((1 - files[file]['label_bias'][f'imbalance_{label_bias}']['original'].accuracy()) - (1 - files[file]['label_bias'][f'imbalance_0.0']['original'].accuracy()))\n",
    "                        relative_fairness[file].append(abs(files[file]['label_bias'][f'imbalance_{label_bias}']['original'].equal_opportunity_difference()) - abs(files[file]['label_bias'][f'imbalance_0.0']['original'].equal_opportunity_difference()))\n",
    "                    elif metric == 'dp':\n",
    "                        relative_error_rates[file].append((1 - files[file]['label_bias'][f'imbalance_{label_bias}']['original'].accuracy()) - (1 - files[file]['label_bias'][f'imbalance_0.0']['original'].accuracy()))\n",
    "                        relative_fairness[file].append(abs(files[file]['label_bias'][f'imbalance_{label_bias}']['original'].equal_opportunity_difference()) - abs(files[file]['label_bias'][f'imbalance_0.0']['original'].statistical_parity_difference()))\n",
    "                else:\n",
    "                    if metric == 'eod':\n",
    "                        relative_error_rates[file].append((1 - files[file]['label_bias'][f'imbalance_{label_bias}']['original'].accuracy()) - (1 - bayes_optimal_results.accuracy()))\n",
    "                        relative_fairness[file].append(abs(files[file]['label_bias'][f'imbalance_{label_bias}']['original'].equal_opportunity_difference()) - abs(bayes_optimal_results.equal_opportunity_difference()))\n",
    "                    elif metric == 'dp':\n",
    "                        relative_error_rates[file].append((1 - files[file]['label_bias'][f'imbalance_{label_bias}']['original'].accuracy()) - (1 - bayes_optimal_results.accuracy()))\n",
    "                        relative_fairness[file].append(abs(files[file]['label_bias'][f'imbalance_{label_bias}']['original'].equal_opportunity_difference()) - abs(bayes_optimal_results.statistical_parity_difference()))\n",
    "            except TypeError as te:\n",
    "                #print(te)\n",
    "                continue\n",
    "    for i in relative_error_rates:\n",
    "        if i not in omitCls:\n",
    "            #ax[3].errorbar(x=max(relative_error_rates[i]), y=max(relative_fairness[i]), label=i, fmt=markers[i], color=colors[i])\n",
    "            ax[1].errorbar(x=np.mean(relative_error_rates[i]), y=np.mean(relative_fairness[i]),  xerr=np.var(relative_error_rates[i]), yerr=np.var(relative_fairness[i]), label=i, fmt=markers[i], color=colors[i])\n",
    "            #ax[1].set_xlim([-0.05,0.5])\n",
    "            #ax[1].set_ylim([-0.05,0.5])\n",
    "    ax[1].set_xlabel('Relative Error Rate')\n",
    "    ax[1].set_ylabel(axisTitles[metric])\n",
    "    ax[1].set_title('Difference wrt. Unbiased distribution', fontsize=9)\n",
    "    \n",
    "    if dataset == 'credit':\n",
    "        plt.legend(bbox_to_anchor=(1.1,1.1))\n",
    "    elif dataset == 'adult':\n",
    "        plt.legend(loc='right', bbox_to_anchor=(2,0.5))\n",
    "    elif dataset == 'compas':\n",
    "        plt.legend(loc='right', bbox_to_anchor=(2,0.5))\n",
    "    elif dataset == 'bank':\n",
    "        plt.legend(loc='right', bbox_to_anchor=(2,0.5))\n",
    "    elif dataset == 'synthetic':\n",
    "        plt.legend(loc='right', bbox_to_anchor=(2,0.5))\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.bar(range(len(cls_freqs)), cls_freqs.values(), tick_label=list(cls_freqs.keys()))\n",
    "    plt.xticks(rotation=90, fontsize=8)\n",
    "    plt.title('Pareto Dominance Frequency', fontsize=9)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec2963b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " max_labelBiasPlots(files, statType='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485ce812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
