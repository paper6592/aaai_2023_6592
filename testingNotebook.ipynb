{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da159a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.sklearn.datasets import fetch_bank, fetch_compas, fetch_adult, fetch_german\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import tqdm as tqdm\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43f7d18",
   "metadata": {},
   "source": [
    "### EDA on all Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "57f742fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X,y,_ = fetch_adult(subset='train', dropna=True)\n",
    "#X,y = fetch_bank(dropna=False)\n",
    "X,y = fetch_german(dropna=True, binary_age=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ac3d7595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>checking_status</th>\n",
       "      <th>duration</th>\n",
       "      <th>credit_history</th>\n",
       "      <th>purpose</th>\n",
       "      <th>credit_amount</th>\n",
       "      <th>savings_status</th>\n",
       "      <th>employment</th>\n",
       "      <th>installment_commitment</th>\n",
       "      <th>other_parties</th>\n",
       "      <th>residence_since</th>\n",
       "      <th>...</th>\n",
       "      <th>age</th>\n",
       "      <th>other_payment_plans</th>\n",
       "      <th>housing</th>\n",
       "      <th>existing_credits</th>\n",
       "      <th>job</th>\n",
       "      <th>num_dependents</th>\n",
       "      <th>own_telephone</th>\n",
       "      <th>foreign_worker</th>\n",
       "      <th>sex</th>\n",
       "      <th>marital_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>no checking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>existing paid</td>\n",
       "      <td>radio/tv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;100</td>\n",
       "      <td>1&lt;=X&lt;4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>own</td>\n",
       "      <td>NaN</td>\n",
       "      <td>skilled</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>yes</td>\n",
       "      <td>male</td>\n",
       "      <td>single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>394</td>\n",
       "      <td>NaN</td>\n",
       "      <td>530</td>\n",
       "      <td>280</td>\n",
       "      <td>NaN</td>\n",
       "      <td>603</td>\n",
       "      <td>339</td>\n",
       "      <td>NaN</td>\n",
       "      <td>907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>814</td>\n",
       "      <td>713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>596</td>\n",
       "      <td>963</td>\n",
       "      <td>690</td>\n",
       "      <td>548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>20.903000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3271.258000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.973000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.845000</td>\n",
       "      <td>...</td>\n",
       "      <td>35.546000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.407000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.155000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>12.058814</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2822.736876</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.118715</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.103718</td>\n",
       "      <td>...</td>\n",
       "      <td>11.375469</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.577654</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.362086</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1365.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2319.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3972.250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18424.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       checking_status     duration credit_history   purpose  credit_amount  \\\n",
       "count             1000  1000.000000           1000      1000    1000.000000   \n",
       "unique               4          NaN              5        10            NaN   \n",
       "top        no checking          NaN  existing paid  radio/tv            NaN   \n",
       "freq               394          NaN            530       280            NaN   \n",
       "mean               NaN    20.903000            NaN       NaN    3271.258000   \n",
       "std                NaN    12.058814            NaN       NaN    2822.736876   \n",
       "min                NaN     4.000000            NaN       NaN     250.000000   \n",
       "25%                NaN    12.000000            NaN       NaN    1365.500000   \n",
       "50%                NaN    18.000000            NaN       NaN    2319.500000   \n",
       "75%                NaN    24.000000            NaN       NaN    3972.250000   \n",
       "max                NaN    72.000000            NaN       NaN   18424.000000   \n",
       "\n",
       "       savings_status employment  installment_commitment other_parties  \\\n",
       "count            1000       1000             1000.000000          1000   \n",
       "unique              5          5                     NaN             3   \n",
       "top              <100     1<=X<4                     NaN          none   \n",
       "freq              603        339                     NaN           907   \n",
       "mean              NaN        NaN                2.973000           NaN   \n",
       "std               NaN        NaN                1.118715           NaN   \n",
       "min               NaN        NaN                1.000000           NaN   \n",
       "25%               NaN        NaN                2.000000           NaN   \n",
       "50%               NaN        NaN                3.000000           NaN   \n",
       "75%               NaN        NaN                4.000000           NaN   \n",
       "max               NaN        NaN                4.000000           NaN   \n",
       "\n",
       "        residence_since  ...          age  other_payment_plans housing  \\\n",
       "count       1000.000000  ...  1000.000000                 1000    1000   \n",
       "unique              NaN  ...          NaN                    3       3   \n",
       "top                 NaN  ...          NaN                 none     own   \n",
       "freq                NaN  ...          NaN                  814     713   \n",
       "mean           2.845000  ...    35.546000                  NaN     NaN   \n",
       "std            1.103718  ...    11.375469                  NaN     NaN   \n",
       "min            1.000000  ...    19.000000                  NaN     NaN   \n",
       "25%            2.000000  ...    27.000000                  NaN     NaN   \n",
       "50%            3.000000  ...    33.000000                  NaN     NaN   \n",
       "75%            4.000000  ...    42.000000                  NaN     NaN   \n",
       "max            4.000000  ...    75.000000                  NaN     NaN   \n",
       "\n",
       "       existing_credits      job num_dependents  own_telephone foreign_worker  \\\n",
       "count       1000.000000     1000    1000.000000           1000           1000   \n",
       "unique              NaN        4            NaN              2              2   \n",
       "top                 NaN  skilled            NaN           none            yes   \n",
       "freq                NaN      630            NaN            596            963   \n",
       "mean           1.407000      NaN       1.155000            NaN            NaN   \n",
       "std            0.577654      NaN       0.362086            NaN            NaN   \n",
       "min            1.000000      NaN       1.000000            NaN            NaN   \n",
       "25%            1.000000      NaN       1.000000            NaN            NaN   \n",
       "50%            1.000000      NaN       1.000000            NaN            NaN   \n",
       "75%            2.000000      NaN       1.000000            NaN            NaN   \n",
       "max            4.000000      NaN       2.000000            NaN            NaN   \n",
       "\n",
       "         sex marital_status  \n",
       "count   1000           1000  \n",
       "unique     2              4  \n",
       "top     male         single  \n",
       "freq     690            548  \n",
       "mean     NaN            NaN  \n",
       "std      NaN            NaN  \n",
       "min      NaN            NaN  \n",
       "25%      NaN            NaN  \n",
       "50%      NaN            NaN  \n",
       "75%      NaN            NaN  \n",
       "max      NaN            NaN  \n",
       "\n",
       "[11 rows x 21 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e2d49745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.drop(['poutcome', 'contact'], inplace=True, axis=1)\n",
    "# X.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ea70735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X.drop(['education-num'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7e3b158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X['income'] = y\n",
    "#X['deposit'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "50b2bcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: Bad credit, 1:Good credit\n",
    "X['credit'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c30fd701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in list(X.index):\n",
    "#     X.loc[i, 'deposit'] = y.loc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bdb02ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     sex     age    foreign_worker\n",
       "0    male    aged   yes                 male\n",
       "1    female  young  yes               female\n",
       "2    male    aged   yes                 male\n",
       "3    male    aged   yes                 male\n",
       "4    male    aged   yes                 male\n",
       "                                       ...  \n",
       "995  female  aged   yes               female\n",
       "996  male    aged   yes                 male\n",
       "997  male    aged   yes                 male\n",
       "998  male    young  yes                 male\n",
       "999  male    aged   yes                 male\n",
       "Name: sex, Length: 1000, dtype: category\n",
       "Categories (2, object): ['female' < 'male']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b9c85b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X['race'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "46829cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats = {}\n",
    "# stats['00'] = []\n",
    "# stats['01'] = []\n",
    "# stats['10'] = []\n",
    "# stats['11'] = []\n",
    "# for i in X.index:\n",
    "#     if X.loc[i]['age'] <= 25 and X.loc[i]['deposit'] == 'yes':\n",
    "#         stats['01'].append(i)\n",
    "#     elif X.loc[i]['age'] <= 25 and X.loc[i]['deposit'] == 'no':\n",
    "#         stats['00'].append(i)\n",
    "#     elif X.loc[i]['age'] > 25 and X.loc[i]['deposit'] == 'yes':\n",
    "#         stats['11'].append(i)\n",
    "#     elif X.loc[i]['age'] > 25 and X.loc[i]['deposit'] == 'no':\n",
    "#         stats['10'].append(i)\n",
    "# stats = {}\n",
    "# stats['00'] = []\n",
    "# stats['01'] = []\n",
    "# stats['10'] = []\n",
    "# stats['11'] = []\n",
    "# for i in X.index:\n",
    "#     if X.loc[i]['race'] == 'African-American' and X.loc[i]['category'] == 'Recidivated':\n",
    "#         stats['01'].append(i)\n",
    "#     elif X.loc[i]['race'] == 'African-American' and X.loc[i]['category'] == 'Survived':\n",
    "#         stats['00'].append(i)\n",
    "#     elif X.loc[i]['race'] == 'Caucasian' and X.loc[i]['category'] == 'Recidivated':\n",
    "#         stats['11'].append(i)\n",
    "#     elif X.loc[i]['race'] == 'Caucasian' and X.loc[i]['category'] == 'Survived':\n",
    "#         stats['10'].append(i)\n",
    "\n",
    "stats = {}\n",
    "stats['00'] = []\n",
    "stats['01'] = []\n",
    "stats['10'] = []\n",
    "stats['11'] = []\n",
    "sensitives = {\n",
    "    'sex':['female', 'male'],\n",
    "    'foreign_worker': ['yes', 'no'],\n",
    "    'age': ['young', 'aged']\n",
    "}\n",
    "attr = 'sex'\n",
    "for i in X.index:\n",
    "    if attr != 'age':\n",
    "        if X.loc[i][attr] == sensitives[attr][0] and X.loc[i]['credit'] == 'good':\n",
    "            stats['01'].append(i)\n",
    "        elif X.loc[i][attr] == sensitives[attr][0] and X.loc[i]['credit'] == 'bad':\n",
    "            stats['00'].append(i)\n",
    "        elif X.loc[i][attr] == sensitives[attr][1] and X.loc[i]['credit'] == 'good':\n",
    "            stats['11'].append(i)\n",
    "        elif X.loc[i][attr] == sensitives[attr][1] and X.loc[i]['credit'] == 'bad':\n",
    "            stats['10'].append(i)\n",
    "    else:\n",
    "        if i[2] == sensitives[attr][0] and X.loc[i]['credit'] == 'good':\n",
    "            stats['01'].append(i)\n",
    "        elif i[2] == sensitives[attr][0] and X.loc[i]['credit'] == 'bad':\n",
    "            stats['00'].append(i)\n",
    "        elif i[2] == sensitives[attr][1] and X.loc[i]['credit'] == 'good':\n",
    "            stats['11'].append(i)\n",
    "        elif i[2] == sensitives[attr][1] and X.loc[i]['credit'] == 'bad':\n",
    "            stats['10'].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d432274f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00 109\n",
      "01 201\n",
      "10 191\n",
      "11 499\n"
     ]
    }
   ],
   "source": [
    "for i in stats:\n",
    "    print(i, len(stats[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6c2f7d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in X.index:\n",
    "#     X.at[i, 'age'] = i[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8be83c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv('/media/data_dump/Mohit/neurips2022_data/credit/raw/original_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b04a2c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainIndices = []\n",
    "testIndices = []\n",
    "for i in stats:\n",
    "    random.shuffle(stats[i])\n",
    "    trainIndices.extend(stats[i][:int(len(stats[i])*0.8)])\n",
    "    testIndices.extend(stats[i][int(len(stats[i])*0.8):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d9ab55aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.loc[trainIndices].to_csv('/media/data_dump/Mohit/neurips2022_data/credit/raw/original_train.csv', index=False)\n",
    "X.loc[testIndices].to_csv('/media/data_dump/Mohit/neurips2022_data/credit/raw/original_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ce0dc500",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X.to_csv('/media/data_dump/Mohit/neurips2022_data/adult/raw/original_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b32ec797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createImbalancedDataframeTabFair(csv_path, balance=True, imbalanceFactor=1.0, dataset='adult', betaSplits=False, beta_neg_factor=None, labelFlip=None):\n",
    "    data = pd.read_csv(csv_path)\n",
    "    xVals = list(data.index)\n",
    "    subgroups = {'00':[], '01':[], '10':[], '11':[]}\n",
    "    for i in xVals:\n",
    "        if dataset == 'adult':\n",
    "            # First index (0) -> Female Gender, Second index(0) -> <=50K, (1) -> >50K\n",
    "            if data.loc[i]['sex'] == 'Female' and data.loc[i]['income'] == '<=50K':\n",
    "                subgroups['00'].append(i)\n",
    "            elif data.loc[i]['sex'] == 'Female' and data.loc[i]['income'] == '>50K':\n",
    "                subgroups['01'].append(i)\n",
    "            elif data.loc[i]['sex'] == 'Male' and data.loc[i]['income'] == '<=50K':\n",
    "                subgroups['10'].append(i)\n",
    "            elif data.loc[i]['sex'] == 'Male' and data.loc[i]['income'] == '>50K':\n",
    "                subgroups['11'].append(i)\n",
    "        elif dataset == 'bank':\n",
    "            # <= 25 -> 0, yes -> 1\n",
    "            if data.loc[i]['age'] <= 25 and data.loc[i]['deposit'] == 'no':\n",
    "                subgroups['00'].append(i)\n",
    "            elif data.loc[i]['age'] <= 25 and data.loc[i]['deposit'] == 'yes':\n",
    "                subgroups['01'].append(i)\n",
    "            elif data.loc[i]['age'] > 25 and data.loc[i]['deposit'] == 'no':\n",
    "                subgroups['10'].append(i)\n",
    "            elif data.loc[i]['age'] > 25 and data.loc[i]['deposit'] == 'yes':\n",
    "                subgroups['11'].append(i)\n",
    "        elif dataset == 'compas':\n",
    "            if data.loc[i]['race'] == 'African-American' and data.loc[i]['category'] == 'Recidivated':\n",
    "                subgroups['01'].append(i)\n",
    "            elif data.loc[i]['race'] == 'African-American' and data.loc[i]['category'] == 'Survived':\n",
    "                subgroups['00'].append(i)\n",
    "            elif data.loc[i]['race'] == 'Caucasian' and data.loc[i]['category'] == 'Recidivated':\n",
    "                subgroups['11'].append(i)\n",
    "            elif data.loc[i]['race'] == 'Caucasian' and data.loc[i]['category'] == 'Survived':\n",
    "                subgroups['10'].append(i)\n",
    "        elif dataset == 'credit':\n",
    "            if data.loc[i]['sex'] == 'female' and data.loc[i]['credit'] == 'good':\n",
    "                subgroups['01'].append(i)\n",
    "            elif data.loc[i]['sex'] == 'female' and data.loc[i]['credit'] == 'bad':\n",
    "                subgroups['00'].append(i)\n",
    "            elif data.loc[i]['sex'] == 'male' and data.loc[i]['credit'] == 'good':\n",
    "                subgroups['11'].append(i)\n",
    "            elif data.loc[i]['sex'] == 'male' and data.loc[i]['credit'] == 'bad':\n",
    "                subgroups['10'].append(i)\n",
    "        elif dataset == 'synthetic':\n",
    "            if data.loc[i]['sensitive'] == 0 and data.loc[i]['label'] == 1:\n",
    "                subgroups['01'].append(i)\n",
    "            elif data.loc[i]['sensitive'] == 0 and data.loc[i]['label'] == 0:\n",
    "                subgroups['00'].append(i)\n",
    "            elif data.loc[i]['sensitive'] == 1 and data.loc[i]['label'] == 1:\n",
    "                subgroups['11'].append(i)\n",
    "            elif data.loc[i]['sensitive'] == 1 and data.loc[i]['label'] == 0:\n",
    "                subgroups['10'].append(i)\n",
    "    if balance == True:\n",
    "        num_points = min(len(subgroups['00']), len(subgroups['01']), len(subgroups['10']), len(subgroups['11']))\n",
    "        for i in subgroups:\n",
    "            subgroups[i] = random.sample(subgroups[i], num_points)\n",
    "    if dataset == 'adult':\n",
    "        imbalanceGroup = '01'\n",
    "        beta_neg_group = '00'\n",
    "        labelFlipIndex = '01'\n",
    "        label = 'income'\n",
    "        flipped_label = '<=50K'\n",
    "    elif dataset == 'bank':\n",
    "        imbalanceGroup = '01'\n",
    "        beta_neg_group = '00'\n",
    "        labelFlipIndex = '01'\n",
    "        label = 'deposit'\n",
    "        flipped_label = 'no'\n",
    "    elif dataset == 'compas':\n",
    "        imbalanceGroup = '11'\n",
    "        beta_neg_group = '10'\n",
    "        labelFlipIndex = '11'\n",
    "        label = 'category'\n",
    "        flipped_label = 'Survived'\n",
    "    elif dataset == 'credit':\n",
    "        imbalanceGroup = '01'\n",
    "        beta_neg_group = '00'\n",
    "        labelFlipIndex = '01'\n",
    "        label = 'credit'\n",
    "        flipped_label = 'bad'\n",
    "    elif dataset == 'synthetic':\n",
    "        imbalanceGroup = '01'\n",
    "        beta_neg_group = '00'\n",
    "        labelFlipIndex = '01'\n",
    "        label = 'label'\n",
    "        flipped_label = 0\n",
    "    if labelFlip is not None:\n",
    "        noisyIndices = random.sample(subgroups[labelFlipIndex], int(len(subgroups[labelFlipIndex])*labelFlip))\n",
    "        for i in noisyIndices:\n",
    "            data.at[i,label]  = flipped_label\n",
    "        return data\n",
    "    else:\n",
    "        if betaSplits == True:\n",
    "            subgroups[imbalanceGroup] = random.sample(subgroups[imbalanceGroup], int(len(subgroups[imbalanceGroup])*imbalanceFactor))\n",
    "            subgroups[beta_neg_group] = random.sample(subgroups[beta_neg_group], int(len(subgroups[beta_neg_group])*beta_neg_factor))\n",
    "        newDataset = subgroups['00'] + subgroups['01'] + subgroups['10'] + subgroups['11']\n",
    "        return data.loc[newDataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a5d7cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "193929f17f2b42478e71641ddea4818b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = 'synthetic'\n",
    "for nu in tqdm(np.arange(0,10)):\n",
    "    nu = nu/10\n",
    "    temp = createImbalancedDataframeTabFair(f'/media/data_dump/Mohit/neurips2022_data/{dataset}/raw/original_train.csv', balance=False, dataset=dataset, labelFlip=nu)\n",
    "    temp.to_csv(f'/media/data_dump/Mohit/neurips2022_data/{dataset}/raw/labelBiasDatasets_train/imbalance_{nu}', index=False)\n",
    "    # Create similarly biased test set\n",
    "    temp = createImbalancedDataframeTabFair(f'/media/data_dump/Mohit/neurips2022_data/{dataset}/raw/original_test.csv', balance=False, dataset=dataset, labelFlip=nu)\n",
    "    temp.to_csv(f'/media/data_dump/Mohit/neurips2022_data/{dataset}/raw/labelBiasDatasets_test/imbalance_{nu}', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0728d85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'synthetic'\n",
    "temp = createImbalancedDataframeTabFair(f'/media/data_dump/Mohit/neurips2022_data/{dataset}/raw/original_test.csv', imbalanceFactor=1.0, balance=True, dataset=dataset)\n",
    "temp.to_csv(f'/media/data_dump/Mohit/neurips2022_data/{dataset}/balanced/balanced_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e34706c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa3ab06205149e4af543b58a8f7bba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = 'synthetic'\n",
    "# for i in tqdm(range(1,101)):\n",
    "#     fact = i/100\n",
    "#     temp = createImbalancedDataframeTabFair(f'/media/data_dump/Mohit/neurips2022_data/{dataset}/raw/original_train.csv', imbalanceFactor=fact, balance=True, dataset='compas')\n",
    "#     temp.to_csv('/media/data_dump/Mohit/neurips2022_data/compas/balanced/realDatasets/imbalance_' + str(i), index=False)\n",
    "for beta_pos in tqdm(np.arange(1,11)):\n",
    "    beta_pos = beta_pos/10\n",
    "    for beta_neg in np.arange(1,11):\n",
    "        beta_neg = beta_neg/10\n",
    "        # Create train set\n",
    "        temp = createImbalancedDataframeTabFair(f'/media/data_dump/Mohit/neurips2022_data/{dataset}/raw/original_train.csv', imbalanceFactor=beta_pos, balance=False, dataset=dataset, betaSplits=True, beta_neg_factor=beta_neg)\n",
    "        temp.to_csv(f'/media/data_dump/Mohit/neurips2022_data/{dataset}/raw/betaDatasets/imbalance_{beta_pos}_{beta_neg}', index=False)\n",
    "        # Create similarly biased test set\n",
    "        temp = createImbalancedDataframeTabFair(f'/media/data_dump/Mohit/neurips2022_data/{dataset}/raw/original_test.csv', imbalanceFactor=beta_pos, balance=False, dataset=dataset, betaSplits=True, beta_neg_factor=beta_neg)\n",
    "        temp.to_csv(f'/media/data_dump/Mohit/neurips2022_data/{dataset}/raw/test_betaDatasets/imbalance_{beta_pos}_{beta_neg}', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15deb0f",
   "metadata": {},
   "source": [
    "### Testing Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff9623de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.inprocessing.exponentiated_gradient_reduction import ExponentiatedGradientReduction\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "279b1069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://inria.github.io/scikit-learn-mooc/python_scripts/03_categorical_pipeline_column_transformer.html\n",
    "def preprocessDataset(train_path='/media/data_dump/Mohit/neurips2022_data/adult/raw/realDatasets/imbalance_1', test_path='/media/data_dump/Mohit/neurips2022_data/adult/raw/original_test.csv', dataset='adult'):\n",
    "    dataset_train = pd.read_csv(train_path)\n",
    "    dataset_test = pd.read_csv(test_path)\n",
    "    if dataset == 'adult':\n",
    "        sensitive='sex'\n",
    "        label='income'\n",
    "        dropAttrs = ['income', 'sex', 'race']\n",
    "        labelReplacementDict = {'<=50K':0, '>50K':1}\n",
    "        sensitiveReplacementDict = {'Female':0, 'Male':1}\n",
    "        privileged_groups = [{'sex':1}]\n",
    "        unprivileged_groups = [{'sex': 0}]\n",
    "        favorable_label = 1\n",
    "        unfavorable_label = 0\n",
    "    elif dataset == 'bank':\n",
    "        sensitive = 'age'\n",
    "        label='deposit'\n",
    "        dropAttrs = ['age', 'deposit']\n",
    "        labelReplacementDict = {'no':0, 'yes':1}\n",
    "        def sensitiveReplacementFunction(x):\n",
    "            if x <= 25:\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "        privileged_groups = [{'age':1}]\n",
    "        unprivileged_groups = [{'age': 0}]\n",
    "        favorable_label = 1\n",
    "        unfavorable_label = 0\n",
    "    elif dataset == 'compas':\n",
    "        sensitive = 'race'\n",
    "        label = 'category'\n",
    "        dropAttrs = ['race', 'category', 'sex']\n",
    "        labelReplacementDict = {'Survived':0, 'Recidivated':1}\n",
    "        sensitiveReplacementDict = {'African-American':0, 'Caucasian':1}\n",
    "        privileged_groups = [{'race':0}]\n",
    "        unprivileged_groups = [{'race': 1}]\n",
    "        favorable_label = 0\n",
    "        unfavorable_label = 1\n",
    "    # Separate out labels and sensitive Attributes\n",
    "    y_train = dataset_train[label]\n",
    "    z_train = dataset_train[sensitive]\n",
    "    x_train = dataset_train.drop(columns=dropAttrs)\n",
    "    y_test = dataset_test[label]\n",
    "    z_test = dataset_test[sensitive]\n",
    "    x_test = dataset_test.drop(columns=dropAttrs)\n",
    "    # Binarize label and sensitive attribute\n",
    "    y_train.replace(labelReplacementDict, inplace=True)\n",
    "    y_test.replace(labelReplacementDict, inplace=True)\n",
    "    if dataset != 'bank':\n",
    "        z_train.replace(sensitiveReplacementDict, inplace=True)\n",
    "        z_test.replace(sensitiveReplacementDict, inplace=True)\n",
    "    else:\n",
    "        z_train = z_train.apply(sensitiveReplacementFunction)\n",
    "        z_test = z_test.apply(sensitiveReplacementFunction)\n",
    "    # Preprocess Data\n",
    "    numerical_columns_selector = selector(dtype_exclude=object)\n",
    "    categorical_columns_selector = selector(dtype_include=object)\n",
    "    numerical_columns = numerical_columns_selector(x_train)\n",
    "    categorical_columns = categorical_columns_selector(x_train)\n",
    "    categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "    numerical_preprocessor = StandardScaler()\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('one-hot-encoder', categorical_preprocessor, categorical_columns),\n",
    "        ('standard_scaler', numerical_preprocessor, numerical_columns)]).fit(x_train)\n",
    "    tempTrain = preprocessor.transform(x_train)\n",
    "    tempTest = preprocessor.transform(x_test)\n",
    "    # Sometimes we get a sparse CSR matrix, sometimes we get a ndarray\n",
    "    if isinstance(tempTrain, np.ndarray):\n",
    "        preprocessed_x_train = pd.DataFrame(tempTrain.tolist())\n",
    "        preprocessed_x_test = pd.DataFrame(tempTest.tolist())\n",
    "    else:\n",
    "        preprocessed_x_train = pd.DataFrame(tempTrain.toarray())\n",
    "        preprocessed_x_test = pd.DataFrame(tempTest.toarray())\n",
    "    preprocessed_x_train[label] = y_train\n",
    "    preprocessed_x_test[label] = y_test\n",
    "    preprocessed_x_train[sensitive] = z_train\n",
    "    preprocessed_x_test[sensitive] = z_test\n",
    "    # Convert to AIF format\n",
    "    binary_train_dataset = BinaryLabelDataset(favorable_label=favorable_label,\n",
    "                                unfavorable_label=unfavorable_label,\n",
    "                                df=preprocessed_x_train,\n",
    "                                label_names=[label],\n",
    "                                protected_attribute_names=[sensitive],\n",
    "                                unprivileged_protected_attributes=unprivileged_groups)\n",
    "    binary_test_dataset = BinaryLabelDataset(favorable_label=favorable_label,\n",
    "                                unfavorable_label=unfavorable_label,\n",
    "                                df=preprocessed_x_test,\n",
    "                                label_names=[label],\n",
    "                                protected_attribute_names=[sensitive],\n",
    "                                unprivileged_protected_attributes=unprivileged_groups)\n",
    "    return binary_train_dataset, binary_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d063bb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.preprocessing import OptimPreproc, Reweighing\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.opt_tools import OptTools\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.distortion_functions import get_distortion_adult, get_distortion_compas\n",
    "from aif360.algorithms.inprocessing import GerryFairClassifier, MetaFairClassifier, PrejudiceRemover\n",
    "from aif360.algorithms.postprocessing import CalibratedEqOddsPostprocessing, RejectOptionClassification, EqOddsPostprocessing\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "480b9ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataset(train_dataset, train_ratio=0.9, dataset='adult'):\n",
    "    if dataset == 'adult':\n",
    "        sensitive='sex'\n",
    "        label='income'\n",
    "        privileged_groups = [{'sex':1}]\n",
    "        unprivileged_groups = [{'sex': 0}]\n",
    "        favorable_label = 1\n",
    "        unfavorable_label = 0\n",
    "    elif dataset == 'bank':\n",
    "        sensitive = 'age'\n",
    "        label='deposit'\n",
    "        privileged_groups = [{'age':1}]\n",
    "        unprivileged_groups = [{'age': 0}]\n",
    "        favorable_label = 1\n",
    "        unfavorable_label = 0\n",
    "    elif dataset == 'compas':\n",
    "        sensitive = 'race'\n",
    "        label = 'category'\n",
    "        privileged_groups = [{'race':0}]\n",
    "        unprivileged_groups = [{'race': 1}]\n",
    "        favorable_label = 0\n",
    "        unfavorable_label = 1\n",
    "    train_dataset = train_dataset.convert_to_dataframe()[0]\n",
    "    subgroupIndices = {\n",
    "        '00':[],\n",
    "        '01':[],\n",
    "        '10':[],\n",
    "        '11':[]\n",
    "    }\n",
    "    for i in list(train_dataset.index):\n",
    "        if train_dataset.loc[i][sensitive] == 0 and train_dataset.loc[i][label] == 0: \n",
    "            subgroupIndices['00'].append(i)\n",
    "        elif train_dataset.loc[i][sensitive] == 0 and train_dataset.loc[i][label] == 1: \n",
    "            subgroupIndices['01'].append(i)\n",
    "        elif train_dataset.loc[i][sensitive] == 1 and train_dataset.loc[i][label] == 0: \n",
    "            subgroupIndices['10'].append(i)\n",
    "        elif train_dataset.loc[i][sensitive] == 1 and train_dataset.loc[i][label] == 1: \n",
    "            subgroupIndices['11'].append(i)\n",
    "    trainIndices = []\n",
    "    valIndices = []\n",
    "    for i in subgroupIndices:\n",
    "        random.shuffle(subgroupIndices[i])\n",
    "        trainIndices.extend(subgroupIndices[i][:int(len(subgroupIndices[i])*train_ratio)])\n",
    "        valIndices.extend(subgroupIndices[i][int(len(subgroupIndices[i])*train_ratio):])\n",
    "    trainDataset = copy.deepcopy(train_dataset).loc[trainIndices]\n",
    "    valDataset = copy.deepcopy(train_dataset).loc[valIndices]\n",
    "    trainDataset = BinaryLabelDataset(favorable_label=favorable_label,\n",
    "                                unfavorable_label=unfavorable_label,\n",
    "                                df=trainDataset,\n",
    "                                label_names=[label],\n",
    "                                protected_attribute_names=[sensitive],\n",
    "                                unprivileged_protected_attributes=unprivileged_groups)\n",
    "    valDataset = BinaryLabelDataset(favorable_label=favorable_label,\n",
    "                                unfavorable_label=unfavorable_label,\n",
    "                                df=valDataset,\n",
    "                                label_names=[label],\n",
    "                                protected_attribute_names=[sensitive],\n",
    "                                unprivileged_protected_attributes=unprivileged_groups)\n",
    "    return trainDataset, valDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fc934f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainClassifiers(train_dataset, test_dataset, dataset='adult'):\n",
    "    if dataset == 'adult':\n",
    "        sensitive='sex'\n",
    "        label='income'\n",
    "        privileged_groups = [{'sex':1}]\n",
    "        unprivileged_groups = [{'sex': 0}]\n",
    "        favorable_label = 1\n",
    "        unfavorable_label = 0\n",
    "    elif dataset == 'bank':\n",
    "        sensitive = 'age'\n",
    "        label='deposit'\n",
    "        privileged_groups = [{'age':1}]\n",
    "        unprivileged_groups = [{'age': 0}]\n",
    "        favorable_label = 1\n",
    "        unfavorable_label = 0\n",
    "    elif dataset == 'compas':\n",
    "        sensitive = 'race'\n",
    "        label = 'category'\n",
    "        privileged_groups = [{'race':0}]\n",
    "        unprivileged_groups = [{'race': 1}]\n",
    "        favorable_label = 0\n",
    "        unfavorable_label = 1\n",
    "    results={}\n",
    "    # Preprocessing Algorithms\n",
    "    results['pre'] = {}\n",
    "    # Skipping Disparate Impact remover since it requires the conditional distributions to be same between training and testing sets\n",
    "    #dis_impact_remover = DisparateImpactRemover(sensitive_attribute='sex')\n",
    "    #transformed_train_dataset = dis_impact_remover.fit_transform(train_dataset)\n",
    "    #transformed_test_dataset = dis_impact_remover.fit_transform(test_dataset)\n",
    "    for i in ['rew', 'optPre']:\n",
    "        if i == 'rew':\n",
    "            preProc = Reweighing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "        elif i == 'optPre':\n",
    "            preProc = OptimPreproc()\n",
    "        preProc.fit(train_dataset)\n",
    "        new_train_dataset = preProc.transform(train_dataset)\n",
    "        cls = LogisticRegression().fit(new_train_dataset.features[:,:-1], new_train_dataset.labels, sample_weight=new_train_dataset.instance_weights)\n",
    "        new_preds_test = cls.predict(test_dataset.features[:,:-1])\n",
    "        pred_test_set = copy.deepcopy(test_dataset)\n",
    "        pred_test_set.labels = new_preds_test\n",
    "        results['pre'][i] = ClassificationMetric(test_dataset, pred_test_set, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "    # Inprocessing Algorithms\n",
    "    results['in'] = {}\n",
    "    for i in ['gerry_dp', 'gerry_eo', 'meta_fair_dp', 'meta_fair_eo', 'prej_remover', 'exp_grad_dp', 'exp_grad_eo']:\n",
    "        if i == 'gerry_dp':\n",
    "            inProc = GerryFairClassifier(fairness_def='SP')\n",
    "        elif i == 'gerry_eo':\n",
    "            inProc = GerryFairClassifier(fairness_def='FP')\n",
    "        elif i == 'meta_fair_dp':\n",
    "            inProc = MetaFairClassifier(sensitive_attr=sensitive, type='sr')\n",
    "        elif i == 'meta_fair_eo':\n",
    "            inProc = MetaFairClassifier(sensitive_attr=sensitive, type='fdr')\n",
    "        elif i == 'prej_remover':\n",
    "            inProc = PrejudiceRemover(sensitive_attr=sensitive, class_attr=label)\n",
    "        elif i == 'exp_grad_dp':\n",
    "            inProc = ExponentiatedGradientReduction(LogisticRegression(), constraints='DemographicParity')\n",
    "        elif i == 'exp_grad_eo':\n",
    "            inProc = ExponentiatedGradientReduction(LogisticRegression(), constaints='EqualizedOdds')\n",
    "        inProc.fit(train_dataset)\n",
    "        pred_test_set = inProc.predict(test_dataset)\n",
    "        results['in'][i] = ClassificationMetric(test_dataset, pred_test_set, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "    # Postprocessing Algorithms\n",
    "    results['post'] = {}\n",
    "    new_train_set, new_val_set = splitDataset(train_dataset, train_ratio=0.8, dataset=dataset)\n",
    "    base_classifier = LogisticRegression()\n",
    "    base_classifier.fit(new_train_set.features[:,:-1], new_train_set.labels)\n",
    "    preds_val = base_classifier.predict(new_val_set.features[:,:-1])\n",
    "    scores_val = base_classifier.predict_proba(new_val_set.features[:,:-1])\n",
    "    pred_val_set = copy.deepcopy(new_val_set)\n",
    "    pred_val_set.labels = preds_val\n",
    "    pred_val_set.scores = scores_val[:,1]\n",
    "    test_dataset.scores = base_classifier.predict_proba(test_dataset.features[:, :-1])[:,1]\n",
    "    for i in ['eq', 'cal_eq', 'reject']:\n",
    "        if i == 'eq':\n",
    "            postProc = EqOddsPostprocessing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "        elif i == 'cal_eq':\n",
    "            postProc = CalibratedEqOddsPostprocessing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "        elif i == 'reject':\n",
    "            postProc = RejectOptionClassification(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups, metric_name=\"Average odds difference\")\n",
    "        postProc.fit(new_val_set, pred_val_set)\n",
    "        pred_test_set = postProc.predict(test_dataset)\n",
    "        results['post'][i] = ClassificationMetric(test_dataset, pred_test_set, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3d763485",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset, test_dataset = preprocessDataset(dataset='bank', train_path='/media/data_dump/Mohit/neurips2022_data/bank/raw/realDatasets/imbalance_1', test_path='/media/data_dump/Mohit/neurips2022_data/bank/raw/original_test.csv')\n",
    "train_dataset, test_dataset = preprocessDataset('/media/data_dump/Mohit/neurips2022_data/bank/raw/betaDatasets/imbalance_0.1_0.1', '/media/data_dump/Mohit/neurips2022_data/bank/raw/original_test.csv', 'bank')\n",
    "#train_dataset, test_dataset = preprocessDataset(dataset='compas', train_path='/media/data_dump/Mohit/neurips2022_data/compas/raw/realDatasets/imbalance_1', test_path='/media/data_dump/Mohit/neurips2022_data/compas/raw/original_test.csv')\n",
    "# dataset = 'adult'\n",
    "# for i in os.listdir(f'/media/data_dump/Mohit/neurips2022_data/{dataset}/raw/realDatasets/'):\n",
    "#     train_dataset, test_dataset = preprocessDataset(dataset=dataset, train_path=f'/media/data_dump/Mohit/neurips2022_data/{dataset}/balanced/realDatasets/{i}', test_path=f'/media/data_dump/Mohit/neurips2022_data/{dataset}/raw/original_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "692042c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_set, new_val_set = splitDataset(train_dataset, 0.8, 'bank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5c0db511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "base_classifier = LogisticRegression()\n",
    "base_classifier.fit(new_train_set.features[:,:-1], new_train_set.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c6e99443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8824074074074074"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_dataset.labels, base_classifier.predict(test_dataset.features[:,:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bd4ff817",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_val = base_classifier.predict(new_val_set.features[:,:-1])\n",
    "preds_scores = base_classifier.predict_proba(new_val_set.features[:,:-1])\n",
    "pred_val_set = copy.deepcopy(new_val_set)\n",
    "pred_val_set.labels = preds_val\n",
    "pred_val_set.scores = preds_scores[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a73e3756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensitive='sex'\n",
    "# label='income'\n",
    "# privileged_groups = [{'sex':1}]\n",
    "# unprivileged_groups = [{'sex': 0}]\n",
    "# favorable_label = 1\n",
    "# unfavorable_label = 0\n",
    "## Bank variables\n",
    "sensitive = 'age'\n",
    "label='deposit'\n",
    "privileged_groups = [{'age':1}]\n",
    "unprivileged_groups = [{'age': 0}]\n",
    "favorable_label = 1\n",
    "unfavorable_label = 0\n",
    "#exp_grad = ExponentiatedGradientReduction(estimator=LogisticRegression(), constraints=\"DemographicParity\", drop_prot_attr=False)\n",
    "#temp_cls = DisparateImpactRemover(sensitive_attribute='sex')\n",
    "#temp_cls = OptimPreproc(OptTools)\n",
    "#temp_cls = GerryFairClassifier()\n",
    "#temp_cls = MetaFairClassifier(sensitive_attr='sex', type='sr')\n",
    "#temp_cls = PrejudiceRemover(sensitive_attr='sex', class_attr='income')\n",
    "#temp_cls = RejectOptionClassification(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups, metric_name=\"Average odds difference\")\n",
    "#temp_cls = CalibratedEqOddsPostprocessing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "temp_cls = EqOddsPostprocessing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "eb90b32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_cls = temp_cls.fit(new_val_set, pred_val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1f110cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds_test = temp_cls.predict(test_dataset)\n",
    "#preds_test_set = copy.deepcopy(test_dataset)\n",
    "#preds_test_set.labels = preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3f178201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# privileged_groups = [{'sex':1}]\n",
    "# unprivileged_groups = [{'sex': 0}]\n",
    "# favorable_label = 1\n",
    "# unfavorable_label = 0\n",
    "sensitive = 'age'\n",
    "label='deposit'\n",
    "privileged_groups = [{'age':1}]\n",
    "unprivileged_groups = [{'age': 0}]\n",
    "favorable_label = 1\n",
    "unfavorable_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4ce8b68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BinaryLabelDatasetMetric(test_dataset, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups).statistical_parity_difference()\n",
    "test_dataset.scores = base_classifier.predict_proba(test_dataset.features[:, :-1])[:,1]\n",
    "pred_test_set = temp_cls.predict(test_dataset)\n",
    "temp = ClassificationMetric(test_dataset, pred_test_set, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c7faafb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0019647180439068546"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.statistical_parity_difference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b7bc79d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12198569175874825"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ClassificationMetric(test_dataset, test_dataset, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups).statistical_parity_difference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b5764fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5002314814814814"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1427c2e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
